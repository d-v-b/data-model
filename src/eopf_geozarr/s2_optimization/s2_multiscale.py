"""
Streaming multiscale pyramid creation for optimized S2 structure.
Uses lazy evaluation to minimize memory usage during dataset preparation.
"""

from collections.abc import Hashable, Mapping
from typing import Any, Literal

import numpy as np
import structlog
import xarray as xr
from dask import delayed
from dask.array import from_delayed
from pyproj import CRS

from eopf_geozarr.conversion import fs_utils
from eopf_geozarr.conversion.geozarr import (
    _create_tile_matrix_limits,
    create_native_crs_tile_matrix_set,
)
from eopf_geozarr.data_api.geozarr.multiscales.zcm import (
    MULTISCALE_CONVENTION,
    ScaleLevelJSON,
)
from eopf_geozarr.data_api.geozarr.types import (
    XARRAY_ENCODING_KEYS,
    XarrayDataArrayEncoding,
)
from eopf_geozarr.s2_optimization.common import DISTRIBUTED_AVAILABLE
from eopf_geozarr.types import OverviewLevelJSON

from .s2_resampling import determine_variable_type, downsample_variable

log = structlog.get_logger()

MultiscalesFlavor = Literal["ogc_tms", "experimental_multiscales_convention"]

pyramid_levels = {
    0: 10,  # Level 0: 10m (native for b02,b03,b04,b08)
    1: 20,  # Level 1: 20m (native for b05,b06,b07,b11,b12,b8a + all quality)
    2: 60,  # Level 2: 60m (native for b01,b09,b10)
    3: 120,  # Level 3: 120m (2x downsampling from 60m)
    4: 360,  # Level 4: 360m (3x downsampling from 120m)
    5: 720,  # Level 5: 720m (2x downsampling from 360m)
}


def get_grid_spacing(
    ds: xr.DataArray, coords: tuple[Hashable, ...]
) -> tuple[float | int, ...]:
    """
    Get the grid spacing of a regularly-gridded DataArray along the specified coordinates.
    """
    result = []
    for coord in coords:
        result.append(np.abs(ds.coords[coord][0].data - ds.coords[coord][1].data))
    return tuple(result)


def create_multiscale_from_datatree(
    dt_input: xr.DataTree,
    output_path: str,
    enable_sharding: bool,
    spatial_chunk: int,
    crs: CRS | None = None,
) -> dict[str, dict]:
    """
    Create multiscale versions preserving original structure.
    Keeps all original groups, adds r120m, r360m, r720m downsampled versions.

    Args:
        dt_input: Input DataTree with original structure
        output_path: Base output path
        enable_sharding: Enable Zarr v3 sharding
        spatial_chunk: Spatial chunk size
        crs: Coordinate Reference System for datasets

    Returns:
        Dictionary of processed groups
    """
    processed_groups = {}

    # Step 1: Copy all original groups as-is
    for group_path in dt_input.groups:
        if group_path == ".":
            continue

        group_node = dt_input[group_path]

        # Skip parent groups that have children (only process leaf groups)
        if hasattr(group_node, "children") and len(group_node.children) > 0:
            continue

        dataset = group_node.to_dataset()

        # Skip empty groups
        if not dataset.data_vars:
            log.info("Skipping empty group: {}", group_path=group_path)
            continue

        log.info("Copying original group: {}", group_path=group_path)

        output_group_path = f"{output_path}{group_path}"

        # Determine if this is a measurement-related resolution group
        group_name = group_path.split("/")[-1]
        is_measurement_group = (
            group_name.startswith("r")
            and group_name.endswith("m")
            and "/measurements/" in group_path
        )

        if is_measurement_group:
            # Measurement groups: apply custom encoding
            encoding = create_measurements_encoding(
                dataset, spatial_chunk=spatial_chunk, enable_sharding=enable_sharding
            )
        else:
            # Non-measurement groups: preserve original chunking
            encoding = create_original_encoding(dataset)
        ds_out = stream_write_dataset(
            dataset,
            output_group_path,
            encoding,
            enable_sharding=enable_sharding,
            crs=crs,
        )
        processed_groups[group_path] = ds_out

    # Step 2: Create downsampled resolution groups ONLY for measurements
    # Find all resolution-based groups under /measurements/ and organize by base path
    resolution_groups = {}
    base_path = "/measurements/reflectance"
    for group_path in processed_groups.keys():
        # Only process groups under /measurements/reflectance
        if not group_path.startswith(base_path):
            continue

        group_name = group_path.split("/")[-1]
        if group_name in ["r10m", "r20m", "r60m"]:
            resolution_groups[group_name] = processed_groups[group_path]

    # Find the coarsest resolution (r60m > r20m > r10m)
    source_dataset = None
    source_resolution = None

    for res in ["r60m", "r20m", "r10m"]:
        if res in resolution_groups:
            source_dataset = resolution_groups[res]
            source_resolution = int(res[1:-1])  # Extract number
            break

    if not source_dataset or source_resolution is None:
        log.info(
            "No source resolution found for downsampling, skipping downsampled levels"
        )
        return processed_groups  # Stop processing if no valid source dataset is found

    log.info(
        "Creating downsampled versions",
        source_dataset=source_dataset,
        source_resolution=source_resolution,
    )

    # Create r120m
    try:
        r120m_path = f"{base_path}/r120m"
        factor = 120 // source_resolution
        log.info("Creating r120m with factor {}", factor=factor)

        r120m_dataset = create_downsampled_resolution_group(
            source_dataset, factor=factor
        )
        if r120m_dataset and len(r120m_dataset.data_vars) > 0:
            output_path_120 = f"{output_path}{r120m_path}"
            log.info("Writing r120m to {}", output_path_120=output_path_120)
            encoding_120 = create_measurements_encoding(
                r120m_dataset, spatial_chunk=spatial_chunk
            )
            ds_120 = stream_write_dataset(
                r120m_dataset,
                output_path_120,
                encoding_120,
                enable_sharding=enable_sharding,
                crs=crs,
            )
            processed_groups[r120m_path] = ds_120
            resolution_groups["r120m"] = ds_120

            # Create r360m from r120m
            try:
                r360m_path = f"{base_path}/r360m"
                log.info("Creating r360m with factor 3")

                r360m_dataset = create_downsampled_resolution_group(
                    r120m_dataset, factor=3
                )

                if r360m_dataset and len(r360m_dataset.data_vars) > 0:
                    output_path_360 = f"{output_path}{r360m_path}"
                    log.info("Writing r360m to {}", output_path_360=output_path_360)
                    encoding_360 = create_measurements_encoding(
                        r360m_dataset, spatial_chunk=spatial_chunk
                    )
                    ds_360 = stream_write_dataset(
                        r360m_dataset,
                        output_path_360,
                        encoding_360,
                        enable_sharding=enable_sharding,
                        crs=crs,
                    )
                    processed_groups[r360m_path] = ds_360
                    resolution_groups["r360m"] = ds_360

                    # Create r720m from r360m
                    try:
                        r720m_path = f"{base_path}/r720m"
                        log.info("    Creating r720m with factor 2")

                        r720m_dataset = create_downsampled_resolution_group(
                            r360m_dataset, factor=2
                        )

                        if r720m_dataset and len(r720m_dataset.data_vars) > 0:
                            output_path_720 = f"{output_path}{r720m_path}"

                            log.info(
                                "    Writing r720m to {}",
                                output_path_720=output_path_720,
                            )
                            encoding_720 = create_measurements_encoding(
                                r720m_dataset,
                                spatial_chunk=spatial_chunk,
                                enable_sharding=enable_sharding,
                            )
                            ds_720 = stream_write_dataset(
                                r720m_dataset,
                                output_path_720,
                                encoding_720,
                                enable_sharding=enable_sharding,
                                crs=crs,
                            )
                            processed_groups[r720m_path] = ds_720
                            resolution_groups["r720m"] = ds_720
                        else:
                            log.info("    r720m dataset is empty, skipping")
                    except Exception as e:
                        log.warning(
                            "Could not create r720m",
                            base_path=base_path,
                            error=str(e),
                        )
                else:
                    log.info("    r360m dataset is empty, skipping")
            except Exception as e:
                log.warning(
                    "Could not create r360m for {}: {}", base_path=base_path, e=e
                )
            # Track r120m for multiscales if created

            log.info("Tracking r120m for multiscales metadata")
        else:
            log.info("r120m dataset is empty, skipping")
    except Exception as e:
        log.warning("Could not create r120m for {}: {}", base_path=base_path, e=e)

    # Step 3: Add multiscales metadata to parent groups
    log.info("Adding multiscales metadata to parent groups")

    dt_multiscale = add_multiscales_metadata_to_parent(
        output_path,
        base_path,
        resolution_groups,
        multiscales_flavor={"ogc_tms", "experimental_multiscales_convention"},
    )
    processed_groups[base_path] = dt_multiscale

    return processed_groups


def create_measurements_encoding(
    dataset: xr.Dataset, *, spatial_chunk: int, enable_sharding: bool = True
) -> dict[str, XarrayDataArrayEncoding]:
    """
    Create optimized encoding for a pyramid level with advanced chunking and sharding.
    """
    encoding: dict[str, XarrayDataArrayEncoding] = {}

    for var_name, var_data in dataset.data_vars.items():
        # start with the original encoding
        var_encoding: XarrayDataArrayEncoding = {}

        chunks: tuple[int, ...] = ()
        if var_data.ndim >= 2:
            height, width = var_data.shape[-2:]

            # Use advanced aligned chunk calculation
            spatial_chunk_aligned = min(
                spatial_chunk,
                calculate_aligned_chunk_size(width, spatial_chunk),
                calculate_aligned_chunk_size(height, spatial_chunk),
            )

            if var_data.ndim == 3:
                # Single file per variable per time: chunk time dimension to 1
                chunks = (1, spatial_chunk_aligned, spatial_chunk_aligned)
            else:
                chunks = (spatial_chunk_aligned, spatial_chunk_aligned)
        else:
            chunks = (min(spatial_chunk, var_data.shape[0]),)

        # Configure encoding - use proper compressor following geozarr.py pattern
        from zarr.codecs import BloscCodec

        compressor = BloscCodec(cname="zstd", clevel=3, shuffle="shuffle", blocksize=0)

        var_encoding["chunks"] = chunks
        var_encoding["compressors"] = (compressor,)

        # Add advanced sharding if enabled - shards match x/y dimensions exactly
        if enable_sharding and var_data.ndim >= 2:
            shard_dims = calculate_simple_shard_dimensions(var_data.shape, chunks)
            var_encoding["shards"] = shard_dims
        else:
            var_encoding["shards"] = None

        # Forward-propagate the existing encoding
        for key in XARRAY_ENCODING_KEYS - {"compressors", "shards", "chunks"}:
            if key in var_data.encoding:
                var_encoding[key] = var_data.encoding[key]  # type: ignore[literal-required]
        if len(set(var_data.encoding.keys()) - XARRAY_ENCODING_KEYS) > 0:
            log.warning(
                f"Unknown encoding keys in {var_name}: {set(var_data.encoding.keys()) - XARRAY_ENCODING_KEYS}"
            )
        encoding[var_name] = var_encoding

    # Add coordinate encoding
    for coord_name in dataset.coords:
        encoding[coord_name] = {"compressors": []}  # type: ignore[typeddict-item]

    return encoding


def calculate_aligned_chunk_size(dimension_size: int, target_chunk: int) -> int:
    """
    Calculate aligned chunk size following geozarr.py logic.

    This ensures good chunk alignment without complex calculations.
    """
    if target_chunk >= dimension_size:
        return dimension_size

    # Find the largest divisor of dimension_size that's close to target_chunk
    best_chunk = target_chunk
    for chunk_candidate in range(target_chunk, max(target_chunk // 2, 1), -1):
        if dimension_size % chunk_candidate == 0:
            best_chunk = chunk_candidate
            break

    return best_chunk


def calculate_simple_shard_dimensions(
    data_shape: tuple[int, ...], chunks: tuple[int, ...]
) -> tuple[int, ...]:
    """
    Calculate shard dimensions that are compatible with chunk dimensions.

    Shard dimensions must be evenly divisible by chunk dimensions for Zarr v3.
    When possible, shards should match x/y dimensions exactly as required.
    """
    shard_dims = []

    for i, (dim_size, chunk_size) in enumerate(zip(data_shape, chunks)):
        if i == 0 and len(data_shape) == 3:
            # First dimension in 3D data (time) - use single time slice per shard
            shard_dims.append(1)
        else:
            # For x/y dimensions, try to use full dimension size
            # But ensure it's divisible by chunk size
            if dim_size % chunk_size == 0:
                # Perfect: full dimension is divisible by chunk
                shard_dims.append(dim_size)
            else:
                # Find the largest multiple of chunk_size that fits
                num_chunks = dim_size // chunk_size
                if num_chunks > 0:
                    shard_size = num_chunks * chunk_size
                    shard_dims.append(shard_size)
                else:
                    # Fallback: use chunk size itself
                    shard_dims.append(chunk_size)

    return tuple(shard_dims)


def add_multiscales_metadata_to_parent(
    output_path: str,
    base_path: str,
    res_groups: Mapping[str, xr.Dataset],
    multiscales_flavor: set[MultiscalesFlavor] = {
        "ogc_tms",
        "experimental_multiscales_convention",
    },
) -> xr.DataTree:
    """Add GeoZarr-compliant multiscales metadata to parent group."""
    # Sort by resolution (finest to coarsest)
    res_order = {
        "r10m": 10,
        "r20m": 20,
        "r60m": 60,
        "r120m": 120,
        "r360m": 360,
        "r720m": 720,
    }

    all_resolutions = sorted(
        set(res_groups.keys()), key=lambda x: res_order.get(x, 999)
    )

    if len(all_resolutions) < 2:
        log.info(
            "Skipping {} - only one resolution available",
            base_path=base_path,
        )
        return

    # Get CRS and bounds from first available dataset (load from output path)
    first_res = all_resolutions[0]
    first_dataset = res_groups[first_res]

    # Get CRS and bounds
    native_crs = first_dataset.rio.crs if hasattr(first_dataset, "rio") else None
    if native_crs is None:
        log.info("No CRS found, skipping multiscales metadata", base_path=base_path)
        return

    native_bounds = (
        first_dataset.rio.bounds() if hasattr(first_dataset, "rio") else None
    )
    if native_bounds is None:
        log.info(
            "No bounds found, skipping multiscales metadata",
            base_path=base_path,
        )
        return

    # Create overview_levels structure with string-based level names
    overview_levels: list[OverviewLevelJSON] = []
    for res_name in all_resolutions:
        # res_meters = res_order[res_name]
        res_meters = float(get_grid_spacing(res_groups[res_name], ("y",))[0])

        dataset = res_groups[res_name]

        if dataset is None:
            continue

        # Get first data variable to extract dimensions
        first_var = next(iter(dataset.data_vars.values()))
        height, width = first_var.shape[-2:]

        # Calculate zoom level (higher resolution = higher zoom)
        tile_width = 256
        zoom_for_width = max(0, int(np.ceil(np.log2(width / tile_width))))
        zoom_for_height = max(0, int(np.ceil(np.log2(height / tile_width))))
        zoom = max(zoom_for_width, zoom_for_height)

        # Calculate scale factor relative to finest resolution
        finest_res_meters = res_order[all_resolutions[0]]
        scale_factor = res_meters // finest_res_meters

        # calculate translation relative to finest resolution
        trans_meters = (res_meters - finest_res_meters) / 2

        overview_levels.append(
            {
                "level": res_name,  # Use string-based level name
                "zoom": zoom,
                "width": width,
                "height": height,
                "translation_relative": trans_meters,
                "scale_absolute": res_meters,
                "scale_relative": scale_factor,
                "chunks": dataset.data_vars[first_var.name].chunks,
            }
        )

    if len(overview_levels) < 2:
        log.info("    Could not create overview levels for {}", base_path=base_path)
        return

    multiscales: dict[str, Any] = {"multiscales": {}}

    if "ogc_tms" in multiscales_flavor:
        # Create tile matrix set using geozarr function
        tile_matrix_set = create_native_crs_tile_matrix_set(
            native_crs,
            native_bounds,
            overview_levels,
            group_prefix=None,
        )

        # Create tile matrix limits
        tile_matrix_limits = _create_tile_matrix_limits(
            overview_levels,
            tile_width=256,
        )
        multiscales["multiscales"].update(
            {
                "tile_matrix_set": tile_matrix_set,
                "resampling_method": "average",
                "tile_matrix_limits": tile_matrix_limits,
            }
        )
    if "experimental_multiscales_convention" in multiscales_flavor:
        scale_levels: list[ScaleLevelJSON] = []
        for overview_level in overview_levels:
            scale_levels.append(
                {
                    "asset": str(overview_level["level"]),
                    "transform": {
                        "scale": (overview_level["scale_relative"],),
                        "translation": (
                            overview_level["translation_relative"],
                            overview_level["translation_relative"],
                        ),
                    },
                }
            )
        multiscales["zarr_conventions_version"] = "0.1.0"
        multiscales["zarr_conventions"] = MULTISCALE_CONVENTION
        multiscales["multiscales"].update(
            {
                "layout": tuple(scale_levels),
                "resampling_method": "average",
            }
        )

    # Create parent group path
    parent_group_path = f"{output_path}{base_path}"
    dt_multiscale = xr.DataTree()
    for res in all_resolutions:
        dt_multiscale[res] = xr.DataTree()
    dt_multiscale.attrs.update(multiscales)

    dt_multiscale.to_zarr(
        parent_group_path,
        mode="a",
        consolidated=False,
        zarr_format=3,
    )

    log.info(
        f"Added {len(overview_levels)} multiscale levels to {base_path}",
    )

    return dt_multiscale


def create_original_encoding(dataset: xr.Dataset) -> dict[str, XarrayDataArrayEncoding]:
    """Write a group preserving its original chunking and encoding."""
    from zarr.codecs import BloscCodec

    # Simple encoding that preserves original structure
    compressor = BloscCodec(cname="zstd", clevel=3, shuffle="shuffle", blocksize=0)
    encoding = {}

    for var_name in dataset.data_vars:
        # start with the original encoding
        var_data = dataset.data_vars[var_name]
        var_encoding: XarrayDataArrayEncoding = {}
        var_encoding["compressors"] = (compressor,)
        for key in XARRAY_ENCODING_KEYS - {"compressors"}:
            if key in var_data.encoding:
                var_encoding[key] = var_data.encoding[key]  # type: ignore[literal-required]
        if len(set(var_data.encoding.keys()) - XARRAY_ENCODING_KEYS) > 0:
            log.warning(
                f"Unknown encoding keys in {var_name}: {set(var_data.encoding.keys()) - XARRAY_ENCODING_KEYS}"
            )
        encoding[var_name] = var_encoding

    for coord_name in dataset.coords:
        encoding[coord_name] = {"compressors": None}

    return encoding


def create_downsampled_resolution_group(
    source_dataset: xr.Dataset, factor: int
) -> xr.Dataset:
    """Create a downsampled version of a dataset by given factor."""
    if not source_dataset or len(source_dataset.data_vars) == 0:
        return xr.Dataset()

    # Get reference dimensions
    ref_var = next(iter(source_dataset.data_vars.values()))
    if ref_var.ndim < 2:
        return xr.Dataset()

    current_height, current_width = ref_var.shape[-2:]
    target_height = current_height // factor
    target_width = current_width // factor

    if target_height < 1 or target_width < 1:
        return xr.Dataset()

    # Downsample all variables using existing lazy operations
    lazy_vars = {}
    for var_name, var_data in source_dataset.data_vars.items():
        if var_data.ndim < 2:
            continue
        var_typ = determine_variable_type(var_name, var_data)
        if var_typ == "quality_mask":
            lazy_downsampled = (
                var_data.coarsen({"x": factor, "y": factor}, boundary="trim")
                .max()
                .sdyupr
            )
        elif var_typ == "reflectance":
            lazy_downsampled = var_data.coarsen(
                {"x": factor, "y": factor}, boundary="trim"
            ).mean()
        elif var_typ == "classification":
            lazy_downsampled = var_data.coarsen(
                {"x": factor, "y": factor}, boundary="trim"
            ).reduce(subsample_2)
        elif var_typ == "probability":
            lazy_downsampled = var_data.coarsen(
                {"x": factor, "y": factor}, boundary="trim"
            ).mean()
        else:
            raise ValueError(f"Unknown variable type {var_typ}")

        # preserve encoding
        lazy_downsampled.encoding = var_data.encoding
        # Ensure that dtype is preserved
        lazy_vars[var_name] = lazy_downsampled.astype(var_data.dtype)

    if not lazy_vars:
        return xr.Dataset()

    # Create dataset with lazy variables and coordinates
    dataset = xr.Dataset(lazy_vars, attrs=source_dataset.attrs)

    return dataset


def subsample_2(a: xr.DataArray, axis: tuple[int, ...] | None = None) -> xr.DataArray:
    if axis is None:
        return a[((slice(None, None, 2),) * a.ndim)]
    else:
        indexer = [
            slice(None, None, 2) if i in axis else slice(None) for i in range(a.ndim)
        ]
        return a[tuple(indexer)]


def create_downsampled_coordinates(
    level_2_dataset: xr.Dataset,
    target_height: int,
    target_width: int,
    downsample_factor: int,
) -> dict[str, Any]:
    """Create downsampled coordinates for higher pyramid levels."""

    # Get original coordinates from level 2
    if "x" not in level_2_dataset.coords or "y" not in level_2_dataset.coords:
        return {}

    x_coords_orig = level_2_dataset.coords["x"].values
    y_coords_orig = level_2_dataset.coords["y"].values

    # Calculate downsampled coordinates by taking every nth point
    # where n is the downsample_factor
    x_coords_downsampled = x_coords_orig[::downsample_factor][:target_width]
    y_coords_downsampled = y_coords_orig[::downsample_factor][:target_height]

    # Create coordinate dictionary with proper attributes
    coords = {}

    # Copy x coordinate with attributes
    x_attrs = level_2_dataset.coords["x"].attrs.copy()
    coords["x"] = (["x"], x_coords_downsampled, x_attrs)

    # Copy y coordinate with attributes
    y_attrs = level_2_dataset.coords["y"].attrs.copy()
    coords["y"] = (["y"], y_coords_downsampled, y_attrs)

    # Copy any other coordinates that might exist
    for coord_name, coord_data in level_2_dataset.coords.items():
        if coord_name not in ["x", "y"]:
            coords[coord_name] = coord_data

    return coords


def create_lazy_downsample_operation_from_existing(
    source_data: xr.DataArray, target_height: int, target_width: int
) -> xr.DataArray:
    """Create lazy downsampling operation from existing data."""

    @delayed  # type: ignore[misc]
    def downsample_operation() -> Any:
        var_type = determine_variable_type(source_data.name, source_data)
        return downsample_variable(source_data, target_height, target_width, var_type)

    # Create delayed operation
    lazy_result = downsample_operation()

    # Estimate output shape and chunks
    output_shape: tuple[int, ...]
    chunks: tuple[int, ...]
    if source_data.ndim == 3:
        output_shape = (source_data.shape[0], target_height, target_width)
        chunks = (1, min(256, target_height), min(256, target_width))
    else:
        output_shape = (target_height, target_width)
        chunks = (min(256, target_height), min(256, target_width))

    # Create Dask array from delayed operation
    dask_array = from_delayed(
        lazy_result, shape=output_shape, dtype=source_data.dtype
    ).rechunk(chunks)

    # Return as xarray DataArray with lazy data - no coords to avoid alignment issues
    # Coordinates will be set when the lazy operation is computed
    return xr.DataArray(
        dask_array,
        dims=source_data.dims,
        attrs=source_data.attrs.copy(),
        name=source_data.name,
    )


def stream_write_dataset(
    dataset: xr.Dataset,
    dataset_path: str,
    encoding: dict[str, XarrayDataArrayEncoding],
    *,
    enable_sharding: bool,
    crs: CRS | None = None,
) -> xr.Dataset:
    """
    Stream write a lazy dataset with advanced chunking and sharding.

    This is where the magic happens: all the lazy downsampling operations
    are executed as the data is streamed to storage with optimal performance.

    Args:
        dataset: Dataset to write
        dataset_path: Output path for dataset
        encoding: Encoding dictionary for variables
        enable_sharding: Enable Zarr v3 sharding
        crs: Coordinate Reference System for geographic metadata

    Returns:
        Written dataset
    """
    # Check if level already exists
    if fs_utils.path_exists(dataset_path):
        log.info(
            "Level path {} already exists. Skipping write.",
            dataset_path=dataset_path,
        )
        existing_ds = xr.open_dataset(
            dataset_path,
            engine="zarr",
            chunks={},
            decode_coords="all",
        )
        return existing_ds

    log.info("Streaming computation and write to {}", dataset_path=dataset_path)
    log.info("Variables", variables=list(dataset.data_vars.keys()))

    # Rechunk dataset to align with encoding
    dataset = rechunk_dataset_for_encoding(dataset, encoding)

    # Add the geo metadata before writing for
    # - /measurements/ groups
    # - /quality/ groups
    if "/measurements/" in dataset_path or "/quality/" in dataset_path:
        write_geo_metadata(dataset, crs=crs)

    # Write with streaming computation and progress tracking
    # The to_zarr operation will trigger all lazy computations
    write_job = dataset.to_zarr(
        dataset_path,
        mode="w",
        consolidated=False,
        zarr_format=3,
        encoding=encoding,
        compute=False,  # Create job first for progress tracking
    )
    write_job = write_job.persist()

    if DISTRIBUTED_AVAILABLE:
        try:
            import distributed

            distributed.progress(write_job, notebook=False)
        except Exception as e:
            log.warning("Could not display progress bar: {}", e=e)
            write_job.compute()
    else:
        log.info("Writing zarr file...")
        write_job.compute()

    log.info("âœ… Streaming write complete for dataset {}", dataset_path=dataset_path)
    return dataset


def write_geo_metadata(
    dataset: xr.Dataset,
    grid_mapping_var_name: str = "spatial_ref",
    crs: CRS | None = None,
) -> None:
    """
    Write geographic metadata to the dataset.

    Args:
        dataset: Dataset to write metadata to
        grid_mapping_var_name: Name for grid mapping variable
        crs: Coordinate Reference System to use (if None, attempts to detect from dataset)
    """
    # Use provided CRS or try to detect from dataset
    if crs is None:
        for var in dataset.data_vars.values():
            if hasattr(var, "rio") and var.rio.crs:
                crs = var.rio.crs
                break
            elif "proj:epsg" in var.attrs:
                epsg = var.attrs["proj:epsg"]
                crs = CRS.from_epsg(epsg)
                break

    if crs is not None:
        dataset.rio.write_crs(
            crs, grid_mapping_name=grid_mapping_var_name, inplace=True
        )
        dataset.rio.write_grid_mapping(grid_mapping_var_name, inplace=True)
        dataset.attrs["grid_mapping"] = grid_mapping_var_name

        for var in dataset.data_vars.values():
            var.rio.write_grid_mapping(grid_mapping_var_name, inplace=True)
            var.attrs["grid_mapping"] = grid_mapping_var_name


def rechunk_dataset_for_encoding(
    dataset: xr.Dataset, encoding: dict[str, XarrayDataArrayEncoding]
) -> xr.Dataset:
    """
    Rechunk dataset variables to align with sharding dimensions when sharding is enabled.

    When using Zarr v3 sharding, Dask chunks must align with shard dimensions to avoid
    checksum validation errors.
    """
    rechunked_vars = {}

    for var_name, var_data in dataset.data_vars.items():
        if var_name in encoding:
            var_encoding = encoding[var_name]

            # If sharding is enabled, rechunk based on shard dimensions
            if "shards" in var_encoding and var_encoding["shards"] is not None:
                target_chunks = var_encoding[
                    "shards"
                ]  # Use shard dimensions for rechunking
            elif "chunks" in var_encoding:
                target_chunks = var_encoding["chunks"]  # Fallback to chunk dimensions
            else:
                # No specific chunking needed, use original variable
                rechunked_vars[var_name] = var_data
                continue

            # Create chunk dict using the actual dimensions of the variable
            var_dims = var_data.dims
            chunk_dict = {}
            for i, dim in enumerate(var_dims):
                if i < len(target_chunks):
                    chunk_dict[dim] = target_chunks[i]

            # Rechunk the variable to match the target dimensions
            rechunked_vars[var_name] = var_data.chunk(chunk_dict)
        else:
            # No specific chunking needed, use original variable
            rechunked_vars[var_name] = var_data

    # Create new dataset with rechunked variables, preserving coordinates
    rechunked_dataset = xr.Dataset(
        rechunked_vars, coords=dataset.coords, attrs=dataset.attrs
    )

    return rechunked_dataset


def extract_scale_offset_encoding(
    attrs: Mapping[str, Mapping[str, object]],
) -> dict[str, object]:
    """
    extract the scale / offset encoding from _eopf_attrs
    """
    encoding = {}
    encoding["add_offset"] = attrs["_eopf_attrs"]["add_offset"]
    encoding["scale_factor"] = attrs["_eopf_attrs"]["scale_factor"]
    encoding["dtype"] = attrs["_eopf_attrs"]["dtype"]
    return encoding
