{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"EOPF GeoZarr Documentation","text":"<p>Welcome to the EOPF GeoZarr library documentation. This library provides tools to convert EOPF (Earth Observation Processing Framework) datasets to GeoZarr-spec 0.4 compliant format while maintaining scientific accuracy and optimizing for cloud-native workflows.</p>"},{"location":"#quick-navigation","title":"Quick Navigation","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation - Install the library and set up your environment</li> <li>Quick Start - Convert your first dataset in minutes</li> <li>User Guide - Comprehensive usage guide with advanced options</li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>API Reference - Complete Python API documentation</li> <li>Examples - Practical examples for common use cases</li> <li>Architecture - Technical architecture and design principles</li> <li>GeoZarr Mini Spec - Implementation-specific GeoZarr specification</li> </ul>"},{"location":"#support","title":"Support","text":"<ul> <li>FAQ - Frequently asked questions and troubleshooting</li> <li>GeoZarr Specification - Our contributions to the GeoZarr spec</li> </ul>"},{"location":"#what-is-eopf-geozarr","title":"What is EOPF GeoZarr?","text":"<p>The EOPF GeoZarr library bridges the gap between EOPF datasets and the emerging GeoZarr specification, enabling:</p> <p>\u2705 Scientific Accuracy - Preserves native CRS and data integrity \u2705 Cloud-Native - Optimized for S3 and distributed processing \u2705 Performance - Intelligent chunking and multiscale pyramids \u2705 Standards Compliant - Full GeoZarr 0.4 and CF conventions support \u2705 Production Ready - Robust error handling and validation  </p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#native-crs-preservation","title":"\ud83c\udf0d Native CRS Preservation","text":"<p>Maintains original coordinate reference systems (UTM, polar stereographic, etc.) without unnecessary reprojection to Web Mercator, preserving scientific accuracy for Earth observation data.</p>"},{"location":"#multiscale-pyramids","title":"\ud83d\udcca Multiscale Pyramids","text":"<p>Automatically generates overview levels with /2 downsampling, creating efficient multiscale pyramids for visualization and analysis at different resolutions.</p>"},{"location":"#intelligent-chunking","title":"\ud83d\udd27 Intelligent Chunking","text":"<p>Implements aligned chunking strategy that prevents partial chunks, optimizes storage efficiency, and improves I/O performance for both local and cloud storage.</p>"},{"location":"#cloud-native-design","title":"\u2601\ufe0f Cloud-Native Design","text":"<p>Full support for AWS S3 and S3-compatible storage with automatic credential detection, retry logic, and optimized multipart uploads.</p>"},{"location":"#standards-compliance","title":"\ud83d\udccb Standards Compliance","text":"<ul> <li>GeoZarr 0.4 specification compliance</li> <li>CF conventions for scientific metadata</li> <li><code>_ARRAY_DIMENSIONS</code> attributes on all arrays</li> <li>Grid mapping variables with proper CRS information</li> <li>Multiscales metadata structure</li> </ul>"},{"location":"#performance-optimized","title":"\ud83d\ude80 Performance Optimized","text":"<ul> <li>Dask integration for distributed processing</li> <li>Lazy loading for memory efficiency</li> <li>Band-by-band processing with validation</li> <li>Retry logic for robust network operations</li> </ul>"},{"location":"#supported-data","title":"Supported Data","text":""},{"location":"#satellite-missions","title":"Satellite Missions","text":"<ul> <li>Sentinel-2 L1C and L2A products (fully supported)</li> <li>Sentinel-1 (planned support)</li> <li>Extensible architecture for additional missions</li> </ul>"},{"location":"#data-formats","title":"Data Formats","text":"<ul> <li>Input: EOPF DataTree (Zarr format)</li> <li>Output: GeoZarr-compliant Zarr with multiscale structure</li> </ul>"},{"location":"#storage-backends","title":"Storage Backends","text":"<ul> <li>Local filesystems</li> <li>AWS S3</li> <li>S3-compatible storage (MinIO, DigitalOcean Spaces, etc.)</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code># test: skip\nimport xarray as xr\nfrom eopf_geozarr import create_geozarr_dataset\n\n# Load EOPF dataset\ndt = xr.open_datatree(\"sentinel2_l2a.zarr\", engine=\"zarr\")\n\n# Convert to GeoZarr\ndt_geozarr = create_geozarr_dataset(\n    dt_input=dt,\n    groups=[\"/measurements/r10m\", \"/measurements/r20m\", \"/measurements/r60m\"],\n    output_path=\"s3://my-bucket/geozarr.zarr\",\n    spatial_chunk=4096\n)\n\nprint(\"Conversion complete!\")\n</code></pre> <p>Or using the command line:</p> <pre><code>eopf-geozarr convert input.zarr s3://bucket/output.zarr\n</code></pre>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<p>The library is organized into focused modules:</p> <ul> <li><code>conversion/</code> - Core conversion engine and algorithms</li> <li><code>geozarr.py</code> - Main conversion functions</li> <li><code>fs_utils.py</code> - Storage backend abstraction</li> <li><code>utils.py</code> - Processing utilities and chunking</li> <li><code>cli.py</code> - Command-line interface</li> <li><code>data_api/</code> - Future data access API (pydantic-zarr integration)</li> </ul>"},{"location":"#implementation-highlights","title":"Implementation Highlights","text":"<p>Based on our experience and contributions to the GeoZarr specification (see ADR-101), this library implements:</p>"},{"location":"#native-crs-tile-matrix-sets","title":"Native CRS Tile Matrix Sets","text":"<p>Creates custom tile matrix sets for arbitrary coordinate reference systems, not just Web Mercator, enabling scientific applications that require native projections.</p>"},{"location":"#aligned-chunking-strategy","title":"Aligned Chunking Strategy","text":"<p>Implements intelligent chunk size calculation that prevents partial chunks and optimizes for both storage efficiency and processing performance.</p>"},{"location":"#hierarchical-data-organization","title":"Hierarchical Data Organization","text":"<p>Uses a sibling-based structure (<code>/0</code>, <code>/1</code>, <code>/2</code>) for resolution levels that complies with xarray DataTree requirements while maintaining GeoZarr specification compliance.</p>"},{"location":"#robust-cloud-integration","title":"Robust Cloud Integration","text":"<p>Production-ready S3 integration with credential validation, error handling, and performance optimization for large-scale Earth observation workflows.</p>"},{"location":"#getting-started_1","title":"Getting Started","text":"<ol> <li>Install the library</li> <li>Quick Start with your first conversion</li> <li>Explore Examples for your specific use case</li> <li>Read the User Guide for advanced usage</li> </ol>"},{"location":"#community-and-support","title":"Community and Support","text":"<ul> <li>Documentation: Comprehensive guides and API reference</li> <li>GitHub: eopf-explorer/data-model</li> <li>Issues: Report bugs and request features</li> <li>Contributions: Help improve the library and specification</li> </ul> <p>The EOPF GeoZarr library is actively developed and maintained by Development Seed, with ongoing contributions to the GeoZarr specification to better serve the Earth observation community.</p>"},{"location":"api-reference/","title":"API Reference","text":"<p>Complete reference for the EOPF GeoZarr library's Python API.</p>"},{"location":"api-reference/#core-functions","title":"Core Functions","text":""},{"location":"api-reference/#create_geozarr_dataset","title":"create_geozarr_dataset","text":"<p>The main function for converting EOPF datasets to GeoZarr format.</p> <pre><code># test: skip\ndef create_geozarr_dataset(\n    dt_input: xr.DataTree,\n    groups: List[str],\n    output_path: str,\n    spatial_chunk: int = 4096,\n    min_dimension: int = 256,\n    tile_width: int = 256,\n    max_retries: int = 3,\n    **storage_kwargs\n) -&gt; xr.DataTree\n</code></pre> <p>Parameters:</p> <ul> <li><code>dt_input</code> (xr.DataTree): Input EOPF DataTree to convert</li> <li><code>groups</code> (List[str]): List of group paths to process (e.g., <code>[\"/measurements/r10m\"]</code>)</li> <li><code>output_path</code> (str): Output path for the GeoZarr dataset (local or S3)</li> <li><code>spatial_chunk</code> (int, optional): Target spatial chunk size. Default: 4096</li> <li><code>min_dimension</code> (int, optional): Minimum dimension size for processing. Default: 256</li> <li><code>tile_width</code> (int, optional): Tile width for multiscale levels. Default: 256</li> <li><code>max_retries</code> (int, optional): Maximum retry attempts for operations. Default: 3</li> <li><code>**storage_kwargs</code>: Additional storage options (S3 credentials, etc.)</li> </ul> <p>Returns:</p> <ul> <li><code>xr.DataTree</code>: The converted GeoZarr-compliant DataTree</li> </ul> <p>Example:</p> <pre><code># test: skip\nimport xarray as xr\nfrom eopf_geozarr import create_geozarr_dataset\n\ndt = xr.open_datatree(\"input.zarr\", engine=\"zarr\")\ndt_geozarr = create_geozarr_dataset(\n    dt_input=dt,\n    groups=[\"/measurements/r10m\", \"/measurements/r20m\"],\n    output_path=\"output.zarr\",\n    spatial_chunk=2048\n)\n</code></pre>"},{"location":"api-reference/#sentinel-2-optimization-functions","title":"Sentinel-2 Optimization Functions","text":""},{"location":"api-reference/#convert_s2_optimized","title":"convert_s2_optimized","text":"<p>Main function for optimized Sentinel-2 conversion with multiscale pyramid generation.</p> <pre><code># test: skip\ndef convert_s2_optimized(\n    dt_input: xr.DataTree,\n    output_path: str,\n    enable_sharding: bool = True,\n    spatial_chunk: int = 256,\n    compression_level: int = 3,\n    validate_output: bool = True,\n    max_retries: int = 3\n) -&gt; xr.DataTree\n</code></pre> <p>Parameters:</p> <ul> <li><code>dt_input</code> (xr.DataTree): Input Sentinel-2 DataTree</li> <li><code>output_path</code> (str): Output path for optimized dataset</li> <li><code>enable_sharding</code> (bool, optional): Enable Zarr v3 sharding. Default: True</li> <li><code>spatial_chunk</code> (int, optional): Spatial chunk size. Default: 256</li> <li><code>compression_level</code> (int, optional): Compression level 1-9. Default: 3</li> <li><code>validate_output</code> (bool, optional): Validate output after conversion. Default: True</li> <li><code>max_retries</code> (int, optional): Maximum retry attempts for operations. Default: 3</li> </ul> <p>Returns:</p> <ul> <li><code>xr.DataTree</code>: Optimized DataTree with multiscale pyramid</li> </ul> <p>Example:</p> <pre><code># test: skip\nfrom eopf_geozarr.s2_optimization.s2_converter import convert_s2_optimized\nimport xarray as xr\n\ndt = xr.open_datatree(\"s2_product.zarr\", engine=\"zarr\")\ndt_optimized = convert_s2_optimized(\n    dt_input=dt,\n    output_path=\"s2_optimized.zarr\",\n    enable_sharding=True,\n    spatial_chunk=256\n)\n</code></pre>"},{"location":"api-reference/#create_multiscale_from_datatree","title":"create_multiscale_from_datatree","text":"<p>Creates multiscale pyramid from DataTree, reusing native resolution groups.</p> <pre><code># test: skip\ndef create_multiscale_from_datatree(\n    dt_input: xr.DataTree,\n    output_path: str,\n    enable_sharding: bool,\n    spatial_chunk: int,\n    crs: CRS | None = None\n) -&gt; dict[str, dict]\n</code></pre> <p>Parameters:</p> <ul> <li><code>dt_input</code> (xr.DataTree): Input DataTree containing native resolution groups (e.g., r10m, r20m, r60m)</li> <li><code>output_path</code> (str): Output path for the multiscale dataset</li> <li><code>enable_sharding</code> (bool): Enable Zarr v3 sharding for improved performance</li> <li><code>spatial_chunk</code> (int): Spatial chunk size for arrays</li> <li><code>crs</code> (CRS | None, optional): Coordinate reference system. If None, CRS is extracted from input</li> </ul> <p>Returns:</p> <ul> <li><code>dict[str, dict]</code>: Nested dictionary structure organizing the multiscale levels:   <pre><code>{\n    \"measurements\": {\n        \"reflectance\": {\n            \"r10m\": Dataset,   # Native 10m resolution\n            \"r20m\": Dataset,   # Native 20m resolution\n            \"r60m\": Dataset,   # Native 60m resolution\n            \"r120m\": Dataset,  # Computed 120m overview\n            \"r360m\": Dataset,  # Computed 360m overview\n            \"r720m\": Dataset   # Computed 720m overview\n        }\n    }\n}\n</code></pre></li> </ul> <p>Example:</p> <pre><code># test: skip\nfrom eopf_geozarr.s2_optimization.s2_multiscale import create_multiscale_from_datatree\nfrom pyproj import CRS\nimport xarray as xr\n\n# Load Sentinel-2 DataTree with native resolutions\ndt = xr.open_datatree(\"s2_input.zarr\", engine=\"zarr\")\n\n# Create multiscale pyramid\nmultiscale_dict = create_multiscale_from_datatree(\n    dt_input=dt,\n    output_path=\"s2_multiscale.zarr\",\n    enable_sharding=True,\n    spatial_chunk=256,\n    crs=CRS.from_epsg(32633)  # UTM Zone 33N\n)\n\n# Access specific resolution level\nr360m_reflectance = multiscale_dict[\"measurements\"][\"reflectance\"][\"r360m\"]\n</code></pre> <p>Note: The S2 optimization uses xarray's built-in <code>.coarsen()</code> method for efficient downsampling operations, providing better integration with lazy evaluation and memory management.</p>"},{"location":"api-reference/#conversion-functions","title":"Conversion Functions","text":""},{"location":"api-reference/#setup_datatree_metadata_geozarr_spec_compliant","title":"setup_datatree_metadata_geozarr_spec_compliant","text":"<p>Sets up GeoZarr-compliant metadata for a DataTree.</p> <pre><code># test: skip\ndef setup_datatree_metadata_geozarr_spec_compliant(\n    dt: xr.DataTree,\n    geozarr_groups: Dict[str, xr.Dataset]\n) -&gt; None\n</code></pre>"},{"location":"api-reference/#write_geozarr_group","title":"write_geozarr_group","text":"<p>Writes a single group to GeoZarr format with proper metadata.</p> <pre><code># test: skip\ndef write_geozarr_group(\n    group_path: str,\n    datasets: Dict[str, xr.Dataset],\n    output_path: str,\n    spatial_chunk: int = 4096,\n    max_retries: int = 3,\n    **storage_kwargs\n) -&gt; None\n</code></pre>"},{"location":"api-reference/#create_geozarr_compliant_multiscales","title":"create_geozarr_compliant_multiscales","text":"<p>Creates multiscales metadata compliant with GeoZarr specification.</p> <pre><code># test: skip\ndef create_geozarr_compliant_multiscales(\n    datasets: Dict[str, xr.Dataset],\n    tile_width: int = 256\n) -&gt; List[Dict[str, Any]]\n</code></pre>"},{"location":"api-reference/#utility-functions","title":"Utility Functions","text":""},{"location":"api-reference/#calculate_aligned_chunk_size","title":"calculate_aligned_chunk_size","text":"<p>Calculates optimal chunk size that aligns with data dimensions.</p> <pre><code># test: skip\ndef calculate_aligned_chunk_size(\n    dimension_size: int,\n    target_chunk_size: int\n) -&gt; int\n</code></pre> <p>Parameters:</p> <ul> <li><code>dimension_size</code> (int): Size of the data dimension</li> <li><code>target_chunk_size</code> (int): Desired chunk size</li> </ul> <p>Returns:</p> <ul> <li><code>int</code>: Optimal aligned chunk size</li> </ul> <p>Example:</p> <pre><code>from eopf_geozarr.conversion.utils import calculate_aligned_chunk_size\n\n# For a 10980x10980 image with target 4096 chunks\nchunk_size = calculate_aligned_chunk_size(10980, 4096)\nprint(chunk_size)  # Returns 3660 (10980 / 3 = 3660)\n</code></pre>"},{"location":"api-reference/#downsample_2d_array","title":"downsample_2d_array","text":"<p>Downsamples a 2D array by factor of 2 using mean aggregation.</p> <pre><code># test: skip\ndef downsample_2d_array(\n    data: np.ndarray,\n    factor: int = 2\n) -&gt; np.ndarray\n</code></pre>"},{"location":"api-reference/#validate_existing_band_data","title":"validate_existing_band_data","text":"<p>Validates existing band data against expected specifications.</p> <pre><code># test: skip\ndef validate_existing_band_data(\n    dataset: xr.Dataset,\n    band_name: str,\n    expected_shape: Tuple[int, ...],\n    expected_chunks: Tuple[int, ...]\n) -&gt; bool\n</code></pre>"},{"location":"api-reference/#file-system-functions","title":"File System Functions","text":""},{"location":"api-reference/#storage-path-utilities","title":"Storage Path Utilities","text":"<pre><code># test: skip\n# Path normalization and validation\ndef normalize_path(path: str) -&gt; str\ndef is_s3_path(path: str) -&gt; bool\ndef parse_s3_path(s3_path: str) -&gt; tuple[str, str]\n\n# Storage options\ndef get_storage_options(path: str, **kwargs: Any) -&gt; Optional[Dict[str, Any]]\ndef get_s3_storage_options(s3_path: str, **s3_kwargs: Any) -&gt; Dict[str, Any]\n</code></pre>"},{"location":"api-reference/#s3-operations","title":"S3 Operations","text":"<pre><code># test: skip\n# S3 store creation and validation\ndef validate_s3_access(s3_path: str, **s3_kwargs: Any) -&gt; tuple[bool, Optional[str]]\ndef s3_path_exists(s3_path: str, **s3_kwargs: Any) -&gt; bool\n\n# S3 metadata operations\ndef write_s3_json_metadata(\n    s3_path: str,\n    metadata: Dict[str, Any],\n    **s3_kwargs: Any\n) -&gt; None\n\ndef read_s3_json_metadata(s3_path: str, **s3_kwargs: Any) -&gt; Dict[str, Any]\n</code></pre>"},{"location":"api-reference/#zarr-operations","title":"Zarr Operations","text":"<pre><code># test: skip\n# Zarr group operations\ndef open_zarr_group(path: str, mode: str = \"r\", **kwargs: Any) -&gt; zarr.Group\ndef open_s3_zarr_group(s3_path: str, mode: str = \"r\", **s3_kwargs: Any) -&gt; zarr.Group\n\n# Metadata consolidation\ndef consolidate_metadata(output_path: str, **storage_kwargs) -&gt; None\nasync def async_consolidate_metadata(output_path: str, **storage_kwargs) -&gt; None\n</code></pre>"},{"location":"api-reference/#metadata-functions","title":"Metadata Functions","text":""},{"location":"api-reference/#coordinate-metadata","title":"Coordinate Metadata","text":"<pre><code># test: skip\ndef _add_coordinate_metadata(ds: xr.Dataset) -&gt; None\n</code></pre> <p>Adds proper coordinate metadata including:</p> <ul> <li><code>_ARRAY_DIMENSIONS</code> attributes</li> <li>CF standard names</li> <li>Coordinate variable attributes</li> </ul>"},{"location":"api-reference/#grid-mapping","title":"Grid Mapping","text":"<pre><code># test: skip\ndef _setup_grid_mapping(ds: xr.Dataset, grid_mapping_var_name: str) -&gt; None\ndef _add_geotransform(ds: xr.Dataset, grid_mapping_var: str) -&gt; None\n</code></pre>"},{"location":"api-reference/#crs-and-tile-matrix","title":"CRS and Tile Matrix","text":"<pre><code># test: skip\ndef create_native_crs_tile_matrix_set(\n    crs: Any,\n    transform: Any,\n    width: int,\n    height: int,\n    tile_width: int = 256\n) -&gt; Dict[str, Any]\n</code></pre> <p>Creates a tile matrix set for native CRS (non-Web Mercator).</p>"},{"location":"api-reference/#overview-generation","title":"Overview Generation","text":""},{"location":"api-reference/#calculate_overview_levels","title":"calculate_overview_levels","text":"<pre><code># test: skip\ndef calculate_overview_levels(\n    width: int,\n    height: int,\n    min_dimension: int = 256\n) -&gt; List[int]\n</code></pre> <p>Calculates appropriate overview levels based on data dimensions.</p>"},{"location":"api-reference/#create_overview_dataset_all_vars","title":"create_overview_dataset_all_vars","text":"<pre><code># test: skip\ndef create_overview_dataset_all_vars(\n    ds: xr.Dataset,\n    overview_factor: int\n) -&gt; xr.Dataset\n</code></pre> <p>Creates overview dataset with all variables downsampled.</p>"},{"location":"api-reference/#error-handling","title":"Error Handling","text":""},{"location":"api-reference/#retry-logic","title":"Retry Logic","text":"<pre><code># test: skip\ndef write_dataset_band_by_band_with_validation(\n    ds: xr.Dataset,\n    output_path: str,\n    max_retries: int = 3,\n    **storage_kwargs\n) -&gt; None\n</code></pre> <p>Writes dataset with robust error handling and retry logic.</p>"},{"location":"api-reference/#constants-and-enums","title":"Constants and Enums","text":""},{"location":"api-reference/#coordinate-attributes","title":"Coordinate Attributes","text":"<pre><code># test: skip\ndef _get_x_coord_attrs() -&gt; Dict[str, Any]\ndef _get_y_coord_attrs() -&gt; Dict[str, Any]\n</code></pre> <p>Returns standard attributes for X and Y coordinates.</p>"},{"location":"api-reference/#grid-mapping-detection","title":"Grid Mapping Detection","text":"<pre><code># test: skip\ndef is_grid_mapping_variable(ds: xr.Dataset, var_name: str) -&gt; bool\n</code></pre> <p>Determines if a variable is a grid mapping variable.</p>"},{"location":"api-reference/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/#basic-conversion","title":"Basic Conversion","text":"<pre><code># test: skip\nimport xarray as xr\nfrom eopf_geozarr import create_geozarr_dataset\n\n# Load and convert\ndt = xr.open_datatree(\"input.zarr\", engine=\"zarr\")\ndt_geozarr = create_geozarr_dataset(\n    dt_input=dt,\n    groups=[\"/measurements/r10m\"],\n    output_path=\"output.zarr\"\n)\n</code></pre>"},{"location":"api-reference/#advanced-s3-usage","title":"Advanced S3 Usage","text":"<pre><code># test: skip\nfrom eopf_geozarr.conversion.fs_utils import (\n    validate_s3_access,\n    get_s3_storage_options\n)\n\n# Validate S3 access\ns3_path = \"s3://my-bucket/data.zarr\"\nis_valid, error = validate_s3_access(s3_path)\n\nif is_valid:\n    # Get storage options\n    storage_opts = get_s3_storage_options(s3_path)\n\n    # Convert with S3\n    dt_geozarr = create_geozarr_dataset(\n        dt_input=dt,\n        groups=[\"/measurements/r10m\"],\n        output_path=s3_path,\n        **storage_opts\n    )\n</code></pre>"},{"location":"api-reference/#custom-chunking","title":"Custom Chunking","text":"<pre><code># test: skip\nfrom eopf_geozarr.conversion.utils import calculate_aligned_chunk_size\n\n# Calculate optimal chunks for your data\nwidth, height = 10980, 10980\noptimal_chunk = calculate_aligned_chunk_size(width, 4096)\n\ndt_geozarr = create_geozarr_dataset(\n    dt_input=dt,\n    groups=[\"/measurements/r10m\"],\n    output_path=\"output.zarr\",\n    spatial_chunk=optimal_chunk\n)\n</code></pre>"},{"location":"api-reference/#type-hints","title":"Type Hints","text":"<p>The library uses comprehensive type hints. Import types as needed:</p> <pre><code># test: skip\nfrom typing import Dict, List, Optional, Tuple, Any\nimport xarray as xr\nimport numpy as np\n</code></pre>"},{"location":"api-reference/#error-types","title":"Error Types","text":"<p>Common exceptions you may encounter:</p> <ul> <li><code>ValueError</code>: Invalid parameters or data</li> <li><code>FileNotFoundError</code>: Missing input files</li> <li><code>PermissionError</code>: Insufficient permissions for S3 or file operations</li> <li><code>zarr.errors.ArrayNotFoundError</code>: Missing Zarr arrays</li> <li><code>xarray.core.common.DataWithCoords</code>: Data structure issues</li> </ul> <p>For detailed error handling examples, see the FAQ.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>This document describes the architecture and design principles of the EOPF GeoZarr library.</p>"},{"location":"architecture/#overview","title":"Overview","text":"<p>The EOPF GeoZarr library is designed to convert EOPF (Earth Observation Processing Framework) datasets to GeoZarr-spec 0.4 compliant format while maintaining scientific accuracy and optimizing performance.</p> <p>This implementation follows our GeoZarr Mini Spec, which defines the specific subset of the GeoZarr specification that this library implements, including implementation-specific details for chunking, CF compliance, and multiscale dataset organization.</p>"},{"location":"architecture/#design-principles","title":"Design Principles","text":""},{"location":"architecture/#1-scientific-integrity-first","title":"1. Scientific Integrity First","text":"<ul> <li>Native CRS Preservation: Maintains original coordinate reference systems to avoid reprojection artifacts</li> <li>Data Accuracy: Preserves original data values without unnecessary transformations</li> <li>Metadata Fidelity: Ensures all scientific metadata is properly transferred and enhanced</li> </ul>"},{"location":"architecture/#2-performance-optimization","title":"2. Performance Optimization","text":"<ul> <li>Aligned Chunking: Optimizes chunk sizes to prevent partial chunks and improve I/O performance</li> <li>Lazy Loading: Uses xarray and Dask for memory-efficient processing</li> <li>Parallel Processing: Supports distributed computing for large datasets</li> </ul>"},{"location":"architecture/#3-cloud-native-design","title":"3. Cloud-Native Design","text":"<ul> <li>Storage Agnostic: Works with local filesystems, S3, and other cloud storage</li> <li>Scalable: Designed for processing large Earth observation datasets</li> <li>Robust: Includes retry logic and error handling for network operations</li> </ul>"},{"location":"architecture/#system-architecture","title":"System Architecture","text":"<pre><code>graph TB\n    A[EOPF DataTree Input] --&gt; B[Conversion Engine]\n    B --&gt; C[GeoZarr Output]\n\n    B --&gt; D[Metadata Processing]\n    B --&gt; E[Spatial Processing]\n    B --&gt; F[Storage Management]\n\n    D --&gt; D1[CF Conventions]\n    D --&gt; D2[Grid Mapping]\n    D --&gt; D3[Multiscales]\n\n    E --&gt; E1[Chunking Strategy]\n    E --&gt; E2[Overview Generation]\n    E --&gt; E3[CRS Handling]\n\n    F --&gt; F1[Local Storage]\n    F --&gt; F2[S3 Storage]\n    F --&gt; F3[Validation]\n</code></pre>"},{"location":"architecture/#core-components","title":"Core Components","text":""},{"location":"architecture/#1-conversion-engine-conversiongeozarrpy","title":"1. Conversion Engine (<code>conversion/geozarr.py</code>)","text":"<p>The main conversion engine orchestrates the transformation process:</p> <pre><code># test: skip\ndef create_geozarr_dataset(\n    dt_input: xr.DataTree,\n    groups: List[str],\n    output_path: str,\n    **kwargs\n) -&gt; xr.DataTree\n</code></pre> <p>Key Functions:</p> <ul> <li><code>setup_datatree_metadata_geozarr_spec_compliant()</code>: Sets up GeoZarr-compliant metadata</li> <li><code>write_geozarr_group()</code>: Writes individual groups with proper structure</li> <li><code>create_geozarr_compliant_multiscales()</code>: Creates multiscales metadata</li> </ul>"},{"location":"architecture/#2-file-system-utilities-conversionfs_utilspy","title":"2. File System Utilities (<code>conversion/fs_utils.py</code>)","text":"<p>Handles storage operations across different backends:</p> <p>Local Storage:</p> <ul> <li>Path normalization and validation</li> <li>Zarr group operations</li> <li>Metadata consolidation</li> </ul> <p>S3 Storage:</p> <ul> <li>S3 path parsing and validation</li> <li>Credential management</li> <li>S3-specific Zarr operations</li> </ul> <p>Key Functions:</p> <ul> <li><code>get_storage_options()</code>: Unified storage configuration</li> <li><code>validate_s3_access()</code>: S3 access validation</li> <li><code>consolidate_metadata()</code>: Metadata consolidation</li> </ul>"},{"location":"architecture/#3-processing-utilities-conversionutilspy","title":"3. Processing Utilities (<code>conversion/utils.py</code>)","text":"<p>Core processing algorithms:</p> <p>Chunking:</p> <pre><code># test: skip\ndef calculate_aligned_chunk_size(\n    dimension_size: int,\n    target_chunk_size: int\n) -&gt; int\n</code></pre> <p>Downsampling:</p> <p>The library uses xarray's built-in <code>.coarsen()</code> method for efficient downsampling operations, providing better integration with lazy evaluation and memory management.</p> <p>Sentinel-2 Optimization:</p> <p>The S2 optimization module uses a functional programming approach with stateless functions for improved testability and maintainability:</p> <pre><code># test: skip\ndef convert_s2_optimized(\n    dt_input: xr.DataTree,\n    output_path: str,\n    **kwargs\n) -&gt; xr.DataTree\n</code></pre>"},{"location":"architecture/#4-command-line-interface-clipy","title":"4. Command Line Interface (<code>cli.py</code>)","text":"<p>Provides user-friendly command-line access:</p> <ul> <li><code>convert</code>: Main conversion command</li> <li><code>validate</code>: GeoZarr compliance validation</li> <li><code>info</code>: Dataset information display</li> </ul>"},{"location":"architecture/#data-flow","title":"Data Flow","text":""},{"location":"architecture/#1-input-processing","title":"1. Input Processing","text":"<pre><code>graph LR\n    A[EOPF DataTree] --&gt; B[Group Selection]\n    B --&gt; C[Metadata Extraction]\n    C --&gt; D[CRS Analysis]\n    D --&gt; E[Dimension Analysis]\n</code></pre> <ol> <li>DataTree Loading: Load EOPF dataset using xarray</li> <li>Group Selection: Select specific measurement groups to process</li> <li>Metadata Extraction: Extract coordinate and variable metadata</li> <li>CRS Analysis: Determine native coordinate reference system</li> <li>Dimension Analysis: Calculate optimal chunking and overview levels</li> </ol>"},{"location":"architecture/#2-conversion-process","title":"2. Conversion Process","text":"<pre><code>graph TB\n    A[Input Dataset] --&gt; B[Prepare Datasets]\n    B --&gt; C[Create Native Resolution]\n    C --&gt; D[Generate Overviews]\n    D --&gt; E[Apply Metadata]\n    E --&gt; F[Write to Storage]\n\n    B --&gt; B1[Chunking Strategy]\n    B --&gt; B2[CRS Preparation]\n\n    D --&gt; D1[Level 1: /2 Factor]\n    D --&gt; D2[Level 2: /4 Factor]\n    D --&gt; D3[Level N: /2^N Factor]\n\n    E --&gt; E1[CF Conventions]\n    E --&gt; E2[Grid Mapping]\n    E --&gt; E3[Multiscales]\n</code></pre>"},{"location":"architecture/#3-output-structure","title":"3. Output Structure","text":"<p>The library creates a hierarchical structure compliant with GeoZarr specification:</p> <pre><code>output.zarr/\n\u251c\u2500\u2500 .zattrs                    # Root attributes with multiscales\n\u251c\u2500\u2500 measurements/\n\u2502   \u251c\u2500\u2500 r10m/                  # Resolution group\n\u2502   \u2502   \u251c\u2500\u2500 .zattrs           # Group attributes\n\u2502   \u2502   \u251c\u2500\u2500 0/                # Native resolution\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 b02/          # Band data\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 b03/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 b04/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 b08/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 x/            # X coordinates\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 y/            # Y coordinates\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 spatial_ref/  # CRS information\n\u2502   \u2502   \u251c\u2500\u2500 1/                # Overview level 1 (/2)\n\u2502   \u2502   \u2514\u2500\u2500 2/                # Overview level 2 (/4)\n\u2502   \u251c\u2500\u2500 r20m/                 # 20m resolution group\n\u2502   \u2514\u2500\u2500 r60m/                 # 60m resolution group\n\u2514\u2500\u2500 .zmetadata                # Consolidated metadata\n</code></pre>"},{"location":"architecture/#metadata-architecture","title":"Metadata Architecture","text":""},{"location":"architecture/#1-cf-conventions-compliance","title":"1. CF Conventions Compliance","text":"<p>The library ensures full CF (Climate and Forecast) conventions compliance:</p> <pre><code># Coordinate variables\nx_attrs = {\n    'standard_name': 'projection_x_coordinate',\n    'long_name': 'x coordinate of projection',\n    'units': 'm',\n    '_ARRAY_DIMENSIONS': ['x']\n}\n\ny_attrs = {\n    'standard_name': 'projection_y_coordinate', \n    'long_name': 'y coordinate of projection',\n    'units': 'm',\n    '_ARRAY_DIMENSIONS': ['y']\n}\n</code></pre>"},{"location":"architecture/#2-grid-mapping-variables","title":"2. Grid Mapping Variables","text":"<p>Each dataset includes proper grid mapping information:</p> <pre><code># test: skip\ngrid_mapping_attrs = {\n    'grid_mapping_name': 'transverse_mercator',  # or appropriate mapping\n    'projected_crs_name': crs.to_string(),\n    'crs_wkt': crs.to_wkt(),\n    'spatial_ref': crs.to_wkt(),\n    'GeoTransform': transform_string\n}\n</code></pre>"},{"location":"architecture/#3-multiscales-metadata","title":"3. Multiscales Metadata","text":"<p>GeoZarr-compliant multiscales structure:</p> <pre><code># test: skip\nmultiscales = [{\n    'version': '0.4',\n    'name': group_name,\n    'type': 'reduce',\n    'metadata': {\n        'method': 'mean',\n        'version': '0.1.0'\n    },\n    'datasets': [\n        {'path': '0', 'pixels_per_tile': tile_width},\n        {'path': '1', 'pixels_per_tile': tile_width},\n        {'path': '2', 'pixels_per_tile': tile_width}\n    ],\n    'coordinateSystem': {\n        'wkid': crs_epsg,\n        'wkt': crs.to_wkt()\n    }\n}]\n</code></pre>"},{"location":"architecture/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/#1-chunking-strategy","title":"1. Chunking Strategy","text":"<p>The library implements intelligent chunking to optimize performance:</p> <pre><code>def calculate_aligned_chunk_size(dimension_size: int, target_chunk_size: int) -&gt; int:\n    \"\"\"Calculate chunk size that divides evenly into dimension size.\"\"\"\n    if target_chunk_size &gt;= dimension_size:\n        return dimension_size\n\n    # Find largest divisor &lt;= target_chunk_size\n    for chunk_size in range(target_chunk_size, 0, -1):\n        if dimension_size % chunk_size == 0:\n            return chunk_size\n    return 1\n</code></pre> <p>Benefits:</p> <ul> <li>Prevents partial chunks that waste storage</li> <li>Improves read/write performance</li> <li>Reduces memory fragmentation</li> <li>Better Dask integration</li> </ul>"},{"location":"architecture/#2-memory-management","title":"2. Memory Management","text":"<p>Lazy Loading:</p> <ul> <li>Uses xarray's lazy loading capabilities</li> <li>Processes data in chunks to manage memory usage</li> <li>Supports out-of-core processing for large datasets</li> </ul> <p>Band-by-Band Processing:</p> <pre><code># test: skip\ndef write_dataset_band_by_band_with_validation(\n    ds: xr.Dataset,\n    output_path: str,\n    max_retries: int = 3\n) -&gt; None\n</code></pre>"},{"location":"architecture/#3-parallel-processing","title":"3. Parallel Processing","text":"<p>Dask Integration:</p> <ul> <li>Supports Dask distributed computing</li> <li>Automatic parallelization of chunk operations</li> <li>Configurable cluster setup</li> </ul> <p>Retry Logic:</p> <ul> <li>Robust error handling for network operations</li> <li>Configurable retry attempts</li> <li>Graceful degradation on failures</li> </ul>"},{"location":"architecture/#storage-architecture","title":"Storage Architecture","text":""},{"location":"architecture/#1-storage-abstraction","title":"1. Storage Abstraction","text":"<p>The library provides a unified interface for different storage backends:</p> <pre><code># test: skip\ndef get_storage_options(path: str, **kwargs) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Get storage options based on path type.\"\"\"\n    if is_s3_path(path):\n        return get_s3_storage_options(path, **kwargs)\n    return None\n</code></pre>"},{"location":"architecture/#2-s3-integration","title":"2. S3 Integration","text":"<p>Features:</p> <ul> <li>Automatic credential detection</li> <li>Custom endpoint support</li> <li>Bucket validation</li> <li>Optimized multipart uploads</li> </ul> <p>Configuration:</p> <pre><code># test: skip\ns3_options = {\n    'key': os.environ.get('AWS_ACCESS_KEY_ID'),\n    'secret': os.environ.get('AWS_SECRET_ACCESS_KEY'),\n    'endpoint_url': os.environ.get('AWS_ENDPOINT_URL'),\n    'region_name': os.environ.get('AWS_DEFAULT_REGION', 'us-east-1')\n}\n</code></pre>"},{"location":"architecture/#3-metadata-consolidation","title":"3. Metadata Consolidation","text":"<p>Zarr metadata consolidation for improved performance:</p> <pre><code>def consolidate_metadata(output_path: str, **storage_kwargs) -&gt; None:\n    \"\"\"Consolidate Zarr metadata for faster access.\"\"\"\n    store = get_zarr_store(output_path, **storage_kwargs)\n    zarr.consolidate_metadata(store)\n</code></pre>"},{"location":"architecture/#error-handling-and-validation","title":"Error Handling and Validation","text":""},{"location":"architecture/#1-input-validation","title":"1. Input Validation","text":"<ul> <li>DataTree structure validation</li> <li>Group existence checks</li> <li>CRS compatibility verification</li> <li>Dimension consistency checks</li> </ul>"},{"location":"architecture/#2-processing-validation","title":"2. Processing Validation","text":"<ul> <li>Chunk alignment verification</li> <li>Memory usage monitoring</li> <li>Progress tracking</li> <li>Intermediate result validation</li> </ul>"},{"location":"architecture/#3-output-validation","title":"3. Output Validation","text":"<ul> <li>GeoZarr specification compliance</li> <li>Metadata completeness checks</li> <li>Data integrity verification</li> <li>Performance metrics collection</li> </ul>"},{"location":"architecture/#extensibility","title":"Extensibility","text":""},{"location":"architecture/#1-plugin-architecture","title":"1. Plugin Architecture","text":"<p>The library is designed to support extensions:</p> <ul> <li>Custom storage backends</li> <li>Additional metadata formats</li> <li>Custom processing algorithms</li> <li>Validation plugins</li> </ul>"},{"location":"architecture/#2-configuration-system","title":"2. Configuration System","text":"<p>Flexible configuration through:</p> <ul> <li>Environment variables</li> <li>Configuration files</li> <li>Runtime parameters</li> <li>Default value inheritance</li> </ul>"},{"location":"architecture/#testing-architecture","title":"Testing Architecture","text":""},{"location":"architecture/#1-unit-tests","title":"1. Unit Tests","text":"<ul> <li>Individual function testing</li> <li>Mock external dependencies</li> <li>Edge case coverage</li> <li>Performance benchmarks</li> </ul>"},{"location":"architecture/#2-integration-tests","title":"2. Integration Tests","text":"<ul> <li>End-to-end conversion workflows</li> <li>Storage backend testing</li> <li>Real dataset processing</li> <li>Cloud environment testing</li> </ul>"},{"location":"architecture/#3-local-test-data","title":"3. Local Test Data","text":"<p>The library uses an efficient testing approach with lightweight JSON-based Zarr groups that contain only the structure and metadata (no chunked array data). This provides:</p> <ul> <li>Faster Test Execution: Tests run locally without downloading large datasets</li> <li>No Remote Dependencies: Eliminates need for network access during testing</li> <li>Lightweight Fixtures: JSON files define Zarr group structure using <code>pydantic-zarr</code></li> </ul> <p>Test fixtures are created from JSON schemas stored in <code>tests/test_data_api/{s1_examples,s2_examples}/</code> directories, making the test suite both comprehensive and efficient.</p>"},{"location":"architecture/#4-validation-tests","title":"4. Validation Tests","text":"<ul> <li>GeoZarr specification compliance</li> <li>Metadata accuracy verification</li> <li>Data integrity checks</li> <li>Performance regression testing</li> </ul> <p>This architecture ensures the EOPF GeoZarr library is robust, performant, and maintainable while meeting the specific needs of Earth observation data processing.</p>"},{"location":"converter/","title":"Using the GeoZarr Converter","text":"<p>The GeoZarr converter provides tools to transform EOPF datasets into GeoZarr-spec 0.4 compliant format. This guide explains how to use the converter effectively.</p>"},{"location":"converter/#command-line-interface","title":"Command Line Interface","text":"<p>The converter can be accessed via the <code>eopf-geozarr</code> command-line tool. Below are some common use cases:</p>"},{"location":"converter/#basic-conversion","title":"Basic Conversion","text":"<p>Convert an EOPF dataset to GeoZarr format:</p> <pre><code>eopf-geozarr convert input.zarr output.zarr\n</code></pre>"},{"location":"converter/#s3-output","title":"S3 Output","text":"<p>Convert and save the output directly to an S3 bucket:</p> <pre><code>eopf-geozarr convert input.zarr s3://my-bucket/output.zarr\n</code></pre>"},{"location":"converter/#parallel-processing","title":"Parallel Processing","text":"<p>Enable parallel processing for large datasets using a Dask cluster:</p> <pre><code>eopf-geozarr convert input.zarr output.zarr --dask-cluster\n</code></pre>"},{"location":"converter/#validation","title":"Validation","text":"<p>Validate the GeoZarr compliance of a dataset:</p> <pre><code>eopf-geozarr validate output.zarr\n</code></pre>"},{"location":"converter/#python-api","title":"Python API","text":"<p>The converter also provides a Python API for programmatic usage:</p>"},{"location":"converter/#example-basic-conversion","title":"Example: Basic Conversion","text":"<pre><code># test: skip\nimport xarray as xr\nfrom eopf_geozarr import create_geozarr_dataset\n\n# Load your EOPF DataTree\ndt = xr.open_datatree(\"path/to/eopf/dataset.zarr\", engine=\"zarr\")\n\n# Convert to GeoZarr format\ndt_geozarr = create_geozarr_dataset(\n    dt_input=dt,\n    groups=[\"/measurements/r10m\", \"/measurements/r20m\", \"/measurements/r60m\"],\n    output_path=\"path/to/output/geozarr.zarr\",\n    spatial_chunk=4096,\n    min_dimension=256,\n    tile_width=256,\n    max_retries=3\n)\n</code></pre>"},{"location":"converter/#example-s3-output","title":"Example: S3 Output","text":"<pre><code>import os\nfrom eopf_geozarr import create_geozarr_dataset\n\n# Configure S3 credentials\nos.environ['AWS_ACCESS_KEY_ID'] = 'your_access_key'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'your_secret_key'\nos.environ['AWS_DEFAULT_REGION'] = 'us-east-1'\n\n# Convert and save to S3\ndt_geozarr = create_geozarr_dataset(\n    dt_input=dt,\n    groups=[\"/measurements/r10m\", \"/measurements/r20m\", \"/measurements/r60m\"],\n    output_path=\"s3://my-bucket/output.zarr\",\n    spatial_chunk=4096,\n    min_dimension=256,\n    tile_width=256,\n    max_retries=3\n)\n</code></pre>"},{"location":"converter/#advanced-features","title":"Advanced Features","text":""},{"location":"converter/#chunk-alignment","title":"Chunk Alignment","text":"<p>The converter ensures proper chunk alignment to optimize storage and prevent data corruption. It uses the <code>calculate_aligned_chunk_size</code> function to determine optimal chunk sizes.</p>"},{"location":"converter/#multiscale-support","title":"Multiscale Support","text":"<p>The converter supports multiscale datasets, creating overview levels with /2 downsampling logic. Each level is stored as a sibling group (e.g., <code>/0</code>, <code>/1</code>, <code>/2</code>).</p>"},{"location":"converter/#native-crs-preservation","title":"Native CRS Preservation","text":"<p>The converter maintains the native coordinate reference system (CRS) of the dataset, avoiding reprojection to Web Mercator.</p>"},{"location":"converter/#sentinel-2-optimized-conversion-v1","title":"Sentinel-2 Optimized Conversion (V1)","text":"<p>The Sentinel-2 optimized converter (<code>convert_s2_optimized</code>) represents the refined approach (V1) that creates an efficient multiscale pyramid by reusing the original multi-resolution data (r10m, r20m, r60m) without duplication, and adding coarser overview levels (r120m, r360m, r720m) for efficient visualization at lower resolutions.</p>"},{"location":"converter/#v0-vs-v1-converter-key-differences","title":"V0 vs V1 Converter: Key Differences","text":"<p>Understanding the structural differences between the old (V0) and new (V1) converter approaches:</p>"},{"location":"converter/#v0-approach-deprecated-create_geozarr_dataset","title":"V0 Approach (Deprecated - <code>create_geozarr_dataset</code>)","text":"<p>Creates pyramids within each resolution group:</p> <pre><code>output.zarr/\n\u2514\u2500\u2500 measurements/\n    \u2514\u2500\u2500 reflectance/\n        \u251c\u2500\u2500 r10m/\n        \u2502   \u251c\u2500\u2500 0/          # Native 10m data\n        \u2502   \u251c\u2500\u2500 1/          # Downsampled to 20m\n        \u2502   \u251c\u2500\u2500 2/          # Downsampled to 40m\n        \u2502   \u251c\u2500\u2500 3/          # Downsampled to 80m\n        \u2502   \u251c\u2500\u2500 4/          # Downsampled to 160m\n        \u2502   \u2514\u2500\u2500 5/          # Downsampled to 320m\n        \u251c\u2500\u2500 r20m/\n        \u2502   \u251c\u2500\u2500 0/          # Native 20m data\n        \u2502   \u251c\u2500\u2500 1/          # Downsampled to 40m\n        \u2502   \u251c\u2500\u2500 2/          # Downsampled to 80m\n        \u2502   \u251c\u2500\u2500 3/          # Downsampled to 160m\n        \u2502   \u2514\u2500\u2500 4/          # Downsampled to 320m\n        \u2514\u2500\u2500 r60m/\n            \u251c\u2500\u2500 0/          # Native 60m data\n            \u251c\u2500\u2500 1/          # Downsampled to 120m\n            \u2514\u2500\u2500 2/          # Downsampled to 240m\n</code></pre> <p>Issues with V0: - Creates redundant data at overlapping resolutions (e.g., r10m/1 \u2248 r20m/0) - Inefficient storage due to duplication - Complex hierarchy with nested levels within each resolution group</p>"},{"location":"converter/#v1-approach-current-convert_s2_optimized","title":"V1 Approach (Current - <code>convert_s2_optimized</code>)","text":"<p>Creates a consolidated pyramid by reusing native resolutions and adding coarser levels:</p> <pre><code>output.zarr/\n\u2514\u2500\u2500 measurements/\n    \u2514\u2500\u2500 reflectance/\n        \u251c\u2500\u2500 r10m/           # Native 10m data (reused as-is)\n        \u251c\u2500\u2500 r20m/           # Native 20m data (reused as-is)\n        \u251c\u2500\u2500 r60m/           # Native 60m data (reused as-is)\n        \u251c\u2500\u2500 r120m/          # Computed from r60m (2x downsampling)\n        \u251c\u2500\u2500 r360m/          # Computed from r120m (3x downsampling)\n        \u2514\u2500\u2500 r720m/          # Computed from r360m (2x downsampling)\n</code></pre> <p>Why these specific resolution levels?</p> <p>The resolution levels are chosen to balance data preservation with storage optimization:</p> <ul> <li>Native ESA resolutions (10m, 20m, 60m): These are the original resolutions delivered by ESA for Sentinel-2 data and are reused as-is to preserve the source data without any loss</li> <li>Computed overview levels (120m, 360m, 720m): These additional levels were specifically chosen because their downsampling factors allow the data to be chunked and sharded in complete pieces, ensuring:</li> <li>120m (2x from 60m): Standard doubling for the first computed overview</li> <li>360m (3x from 120m): Selected for optimal chunking alignment</li> <li>720m (2x from 360m): Final level for global-scale visualization</li> </ul> <p>This approach maintains the integrity of ESA's original multi-resolution data while adding computationally efficient overview levels for performance at coarser scales.</p> <p>Benefits of V1: - No data duplication - native resolutions are reused directly - More efficient storage - Simpler, flatter hierarchy - Natural fit for Sentinel-2's multi-resolution data model</p>"},{"location":"converter/#key-capabilities","title":"Key Capabilities","text":"<ul> <li>Smart Resolution Consolidation: Combines Sentinel-2's native multi-resolution structure (10m, 20m, 60m) into a unified multiscale pyramid</li> <li>Non-Duplicative Downsampling: Reuses original resolution data instead of recreating it, adding only the coarser levels (120m, 360m, 720m)</li> <li>Variable-Aware Processing: Applies appropriate resampling methods for different data types (reflectance, classification, quality masks, probabilities)</li> <li>Efficient Testing: Improved test infrastructure for faster local development</li> </ul> <p>Note: The V0 converter (<code>create_geozarr_dataset</code>) is deprecated and will be removed in future versions. All new projects should use the V1 converter (<code>convert_s2_optimized</code>).</p>"},{"location":"converter/#usage-example","title":"Usage Example","text":"<pre><code>from eopf_geozarr.s2_optimization.s2_converter import convert_s2_optimized\nimport xarray as xr\n\n# Load Sentinel-2 DataTree\ndt_input = xr.open_datatree(\"path/to/s2/product.zarr\", engine=\"zarr\")\n\n# Convert to optimized multiscale structure\ndt_optimized = convert_s2_optimized(\n    dt_input=dt_input,\n    output_path=\"path/to/output/optimized.zarr\",\n    enable_sharding=True,\n    spatial_chunk=256,\n    compression_level=3,\n    validate_output=True\n)\n</code></pre> <p>The result is a space-efficient multiscale pyramid: <code>/measurements/reflectance/{r10m, r20m, r60m, r120m, r360m, r720m}</code> where the native resolutions are preserved as-is and only the coarser levels are computed.</p>"},{"location":"converter/#error-handling","title":"Error Handling","text":"<p>The converter includes robust error handling and retry logic for network operations, ensuring reliable processing even in challenging environments.</p> <p>For more details, refer to the API Reference.</p>"},{"location":"examples/","title":"Examples","text":"<p>Practical examples demonstrating common use cases for the EOPF GeoZarr library.</p>"},{"location":"examples/#basic-examples","title":"Basic Examples","text":""},{"location":"examples/#simple-local-conversion","title":"Simple Local Conversion","text":"<p>Convert a local EOPF dataset to GeoZarr format:</p> <pre><code># test: skip\nimport xarray as xr\nfrom eopf_geozarr import create_geozarr_dataset\n\n# Load EOPF dataset\ndt = xr.open_datatree(\"sentinel2_l2a.zarr\", engine=\"zarr\")\n\n# Convert to GeoZarr\ndt_geozarr = create_geozarr_dataset(\n    dt_input=dt,\n    groups=[\"/measurements/r10m\"],\n    output_path=\"sentinel2_geozarr.zarr\"\n)\n\nprint(\"Conversion completed successfully!\")\n</code></pre>"},{"location":"examples/#command-line-conversion","title":"Command Line Conversion","text":"<pre><code># Basic conversion\neopf-geozarr convert input.zarr output.zarr\n\n# With custom chunk size\neopf-geozarr convert input.zarr output.zarr --spatial-chunk 2048\n\n# Validate the result\neopf-geozarr validate output.zarr\n</code></pre>"},{"location":"examples/#sentinel-2-examples","title":"Sentinel-2 Examples","text":""},{"location":"examples/#multi-resolution-sentinel-2-processing","title":"Multi-Resolution Sentinel-2 Processing","text":"<p>Process all resolution groups from a Sentinel-2 L2A dataset:</p> <pre><code>import xarray as xr\nfrom eopf_geozarr import create_geozarr_dataset\n\n# Load Sentinel-2 L2A dataset\ndt = xr.open_datatree(\"S2A_MSIL2A_20230615T103031_N0509_R108_T32TQM_20230615T170304.zarr\", \n                      engine=\"zarr\")\n\n# Convert all resolution groups\ndt_geozarr = create_geozarr_dataset(\n    dt_input=dt,\n    groups=[\n        \"/measurements/r10m\",  # B02, B03, B04, B08\n        \"/measurements/r20m\",  # B05, B06, B07, B8A, B11, B12\n        \"/measurements/r60m\"   # B01, B09, B10\n    ],\n    output_path=\"s2_l2a_geozarr.zarr\",\n    spatial_chunk=4096,\n    min_dimension=256\n)\n\n# Inspect the result\nprint(f\"Groups created: {list(dt_geozarr.groups)}\")\nfor group_name in dt_geozarr.groups:\n    group = dt_geozarr[group_name]\n    if hasattr(group, 'ds') and group.ds is not None:\n        print(f\"{group_name}: {dict(group.ds.dims)}\")\n</code></pre>"},{"location":"examples/#sentinel-2-band-analysis","title":"Sentinel-2 Band Analysis","text":"<p>Note on V0 vs V1: This example shows both V0 (deprecated) and V1 (current) approaches. See converter documentation for structural differences.</p>"},{"location":"examples/#v1-approach-recommended-convert_s2_optimized","title":"V1 Approach (Recommended - <code>convert_s2_optimized</code>)","text":"<p>Access bands from the consolidated pyramid structure:</p> <pre><code>import xarray as xr\nimport matplotlib.pyplot as plt\nfrom eopf_geozarr.s2_optimization.s2_converter import convert_s2_optimized\n\n# Convert using V1 optimizer (recommended)\ndt_input = xr.open_datatree(\"s2_l2a_input.zarr\", engine=\"zarr\")\ndt = convert_s2_optimized(\n    dt_input=dt_input,\n    output_path=\"s2_l2a_v1.zarr\",\n    spatial_chunk=256\n)\n\n# Access data from different resolution levels\nds_10m = dt[\"/measurements/reflectance/r10m\"].ds   # Native 10m\nds_20m = dt[\"/measurements/reflectance/r20m\"].ds   # Native 20m\nds_60m = dt[\"/measurements/reflectance/r60m\"].ds   # Native 60m\nds_120m = dt[\"/measurements/reflectance/r120m\"].ds # Computed 120m\n\n# Extract RGB bands for visualization (10m resolution)\nred = ds_10m[\"b04\"]    # Red band\ngreen = ds_10m[\"b03\"]  # Green band  \nblue = ds_10m[\"b02\"]   # Blue band\n\n# Create RGB composite\nrgb = xr.concat([red, green, blue], dim=\"band\")\n\n# Plot comparison of different resolutions\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n# 10m resolution\nrgb.plot.imshow(ax=axes[0], robust=True)\naxes[0].set_title(\"10m Resolution (Native)\")\n\n# 20m resolution (reused native data)\nrgb_20m = xr.concat([ds_20m[\"b04\"], ds_20m[\"b03\"], ds_20m[\"b02\"]], dim=\"band\")\nrgb_20m.plot.imshow(ax=axes[1], robust=True)\naxes[1].set_title(\"20m Resolution (Native)\")\n\n# 60m resolution (reused native data)\nrgb_60m = xr.concat([ds_60m[\"b04\"], ds_60m[\"b03\"], ds_60m[\"b02\"]], dim=\"band\")\nrgb_60m.plot.imshow(ax=axes[2], robust=True)\naxes[2].set_title(\"60m Resolution (Native)\")\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/#v0-approach-deprecated-create_geozarr_dataset","title":"V0 Approach (Deprecated - <code>create_geozarr_dataset</code>)","text":"<p>For reference, the V0 structure with nested pyramid levels:</p> <pre><code>import xarray as xr\nimport matplotlib.pyplot as plt\n\n# Open V0 converted GeoZarr dataset (deprecated structure)\ndt = xr.open_datatree(\"s2_l2a_v0.zarr\", engine=\"zarr\")\n\n# Access 10m resolution with nested pyramid levels\nds_10m_native = dt[\"/measurements/r10m/0\"].ds    # Level 0: native 10m\nds_10m_level1 = dt[\"/measurements/r10m/1\"].ds    # Level 1: downsampled to ~20m\nds_10m_level2 = dt[\"/measurements/r10m/2\"].ds    # Level 2: downsampled to ~40m\n\n# Note: This creates redundant data since r10m/1 \u2248 r20m/0\n</code></pre> <p>Migration Note: V0 is deprecated. Use V1 (<code>convert_s2_optimized</code>) for new projects.</p>"},{"location":"examples/#cloud-storage-examples","title":"Cloud Storage Examples","text":""},{"location":"examples/#aws-s3-integration","title":"AWS S3 Integration","text":"<p>Complete workflow with S3 input and output:</p> <pre><code>import os\nimport xarray as xr\nfrom eopf_geozarr import create_geozarr_dataset\nfrom eopf_geozarr.conversion.fs_utils import validate_s3_access\n\n# Configure AWS credentials\nos.environ['AWS_ACCESS_KEY_ID'] = 'your_access_key'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'your_secret_key'\nos.environ['AWS_DEFAULT_REGION'] = 'us-east-1'\n\n# Define paths\ninput_path = \"s3://sentinel-data/input.zarr\"\noutput_path = \"s3://processed-data/output.zarr\"\n\n# Validate S3 access\nis_valid, error = validate_s3_access(output_path)\nif not is_valid:\n    raise RuntimeError(f\"S3 access validation failed: {error}\")\n\n# Load from S3\ndt = xr.open_datatree(input_path, engine=\"zarr\")\n\n# Convert and save to S3\ndt_geozarr = create_geozarr_dataset(\n    dt_input=dt,\n    groups=[\"/measurements/r10m\", \"/measurements/r20m\"],\n    output_path=output_path,\n    spatial_chunk=2048\n)\n\nprint(f\"Successfully converted and saved to {output_path}\")\n</code></pre>"},{"location":"examples/#s3-with-custom-credentials","title":"S3 with Custom Credentials","text":"<p>Using custom S3 credentials and endpoint:</p> <pre><code>from eopf_geozarr import create_geozarr_dataset\nfrom eopf_geozarr.conversion.fs_utils import get_s3_storage_options\n\n# Custom S3 configuration\ns3_config = {\n    'key': 'custom_access_key',\n    'secret': 'custom_secret_key',\n    'endpoint_url': 'https://s3.custom-provider.com',\n    'region_name': 'eu-west-1'\n}\n\n# Get storage options\nstorage_opts = get_s3_storage_options(\"s3://custom-bucket/output.zarr\", **s3_config)\n\n# Convert with custom S3 settings\ndt_geozarr = create_geozarr_dataset(\n    dt_input=dt,\n    groups=[\"/measurements/r10m\"],\n    output_path=\"s3://custom-bucket/output.zarr\",\n    **storage_opts\n)\n</code></pre>"},{"location":"examples/#performance-optimization-examples","title":"Performance Optimization Examples","text":""},{"location":"examples/#large-dataset-processing-with-dask","title":"Large Dataset Processing with Dask","text":"<p>Process large datasets efficiently using Dask:</p> <pre><code>import xarray as xr\nfrom dask.distributed import Client, LocalCluster\nfrom eopf_geozarr import create_geozarr_dataset\n\n# Set up Dask cluster\ncluster = LocalCluster(n_workers=4, threads_per_worker=2, memory_limit='4GB')\nclient = Client(cluster)\n\ntry:\n    # Load large dataset\n    dt = xr.open_datatree(\"large_sentinel2.zarr\", engine=\"zarr\")\n\n    # Process with optimized chunking for Dask\n    dt_geozarr = create_geozarr_dataset(\n        dt_input=dt,\n        groups=[\"/measurements/r10m\", \"/measurements/r20m\", \"/measurements/r60m\"],\n        output_path=\"large_geozarr.zarr\",\n        spatial_chunk=2048,  # Smaller chunks for distributed processing\n        max_retries=5\n    )\n\n    print(\"Large dataset processing completed!\")\n\nfinally:\n    client.close()\n    cluster.close()\n</code></pre>"},{"location":"examples/#memory-efficient-processing","title":"Memory-Efficient Processing","text":"<p>Process datasets with limited memory:</p> <pre><code>from eopf_geozarr import create_geozarr_dataset\nfrom eopf_geozarr.conversion.utils import calculate_aligned_chunk_size\n\n# Calculate memory-efficient chunk size\ndata_width, data_height = 10980, 10980\nmemory_limit_mb = 512  # 512 MB limit\n\n# Estimate chunk size for memory constraint\n# Assuming float32 data (4 bytes per pixel)\npixels_per_mb = (1024 * 1024) // 4\ntarget_chunk = int((pixels_per_mb * memory_limit_mb) ** 0.5)\n\n# Align chunk size with data dimensions\noptimal_chunk = calculate_aligned_chunk_size(data_width, target_chunk)\n\nprint(f\"Using chunk size: {optimal_chunk}\")\n\n# Process with memory-efficient settings\ndt_geozarr = create_geozarr_dataset(\n    dt_input=dt,\n    groups=[\"/measurements/r10m\"],\n    output_path=\"memory_efficient.zarr\",\n    spatial_chunk=optimal_chunk\n)\n</code></pre>"},{"location":"examples/#advanced-use-cases","title":"Advanced Use Cases","text":""},{"location":"examples/#custom-metadata-enhancement","title":"Custom Metadata Enhancement","text":"<p>Add custom metadata to the converted dataset:</p> <pre><code>import xarray as xr\nfrom eopf_geozarr import create_geozarr_dataset\n\n# Convert dataset\ndt_geozarr = create_geozarr_dataset(\n    dt_input=dt,\n    groups=[\"/measurements/r10m\"],\n    output_path=\"enhanced.zarr\"\n)\n\n# Add custom metadata\ndt_geozarr.attrs.update({\n    'processing_date': '2024-01-15',\n    'processing_software': 'eopf-geozarr v0.1.0',\n    'custom_parameter': 'value'\n})\n\n# Add group-specific metadata\nfor group_name in dt_geozarr.groups:\n    group = dt_geozarr[group_name]\n    if hasattr(group, 'ds') and group.ds is not None:\n        group.ds.attrs['processing_level'] = 'L2A_GeoZarr'\n\n# Save enhanced metadata\ndt_geozarr.to_zarr(\"enhanced.zarr\", mode=\"a\")\n</code></pre>"},{"location":"examples/#validation-and-quality-control","title":"Validation and Quality Control","text":"<p>Comprehensive validation workflow:</p> <pre><code>import xarray as xr\nfrom eopf_geozarr import create_geozarr_dataset\nfrom eopf_geozarr.conversion.utils import validate_existing_band_data\n\n# Convert dataset\ndt_geozarr = create_geozarr_dataset(\n    dt_input=dt,\n    groups=[\"/measurements/r10m\"],\n    output_path=\"validated.zarr\"\n)\n\n# Validate the conversion\ndt_check = xr.open_datatree(\"validated.zarr\", engine=\"zarr\")\n\n# Check multiscales metadata\nmultiscales = dt_check.attrs.get('multiscales', [])\nprint(f\"Multiscales levels: {len(multiscales)}\")\n\n# Validate each resolution level\nfor level in [\"0\", \"1\", \"2\"]:\n    group_path = f\"/measurements/r10m/{level}\"\n    if group_path in dt_check.groups:\n        ds = dt_check[group_path].ds\n        print(f\"Level {level}: {dict(ds.dims)}\")\n\n        # Check required attributes\n        for var_name in ds.data_vars:\n            var = ds[var_name]\n            has_dims = '_ARRAY_DIMENSIONS' in var.attrs\n            has_grid_mapping = 'grid_mapping' in var.attrs\n            print(f\"  {var_name}: dims={has_dims}, grid_mapping={has_grid_mapping}\")\n\n# Validate CRS information\nfor group_name in dt_check.groups:\n    group = dt_check[group_name]\n    if hasattr(group, 'ds') and group.ds is not None:\n        crs_vars = [v for v in group.ds.data_vars if 'spatial_ref' in v or 'crs' in v]\n        print(f\"{group_name} CRS variables: {crs_vars}\")\n</code></pre>"},{"location":"examples/#batch-processing","title":"Batch Processing","text":"<p>Process multiple datasets in batch:</p> <pre><code>import os\nfrom pathlib import Path\nfrom eopf_geozarr import create_geozarr_dataset\nimport xarray as xr\n\ndef batch_convert_datasets(input_dir: str, output_dir: str, groups: list):\n    \"\"\"Convert multiple EOPF datasets to GeoZarr format.\"\"\"\n\n    input_path = Path(input_dir)\n    output_path = Path(output_dir)\n    output_path.mkdir(exist_ok=True)\n\n    # Find all .zarr directories\n    zarr_files = list(input_path.glob(\"*.zarr\"))\n\n    for zarr_file in zarr_files:\n        try:\n            print(f\"Processing {zarr_file.name}...\")\n\n            # Load dataset\n            dt = xr.open_datatree(str(zarr_file), engine=\"zarr\")\n\n            # Convert to GeoZarr\n            output_file = output_path / f\"{zarr_file.stem}_geozarr.zarr\"\n            dt_geozarr = create_geozarr_dataset(\n                dt_input=dt,\n                groups=groups,\n                output_path=str(output_file),\n                spatial_chunk=4096\n            )\n\n            print(f\"\u2713 Completed {zarr_file.name}\")\n\n        except Exception as e:\n            print(f\"\u2717 Failed {zarr_file.name}: {e}\")\n\n# Usage\nbatch_convert_datasets(\n    input_dir=\"/data/sentinel2/raw\",\n    output_dir=\"/data/sentinel2/geozarr\",\n    groups=[\"/measurements/r10m\", \"/measurements/r20m\"]\n)\n</code></pre>"},{"location":"examples/#integration-examples","title":"Integration Examples","text":""},{"location":"examples/#stac-integration","title":"STAC Integration","text":"<p>Create STAC items for converted GeoZarr datasets:</p> <pre><code>import json\nfrom datetime import datetime\nimport xarray as xr\nfrom eopf_geozarr import create_geozarr_dataset\n\n# Convert dataset\ndt_geozarr = create_geozarr_dataset(\n    dt_input=dt,\n    groups=[\"/measurements/r10m\"],\n    output_path=\"stac_ready.zarr\"\n)\n\n# Extract metadata for STAC\nds = dt_geozarr[\"/measurements/r10m/0\"].ds\nspatial_ref = ds.get('spatial_ref', ds.get('crs', None))\n\n# Create basic STAC item\nstac_item = {\n    \"stac_version\": \"1.0.0\",\n    \"type\": \"Feature\",\n    \"id\": \"sentinel2_geozarr_example\",\n    \"properties\": {\n        \"datetime\": datetime.now().isoformat(),\n        \"platform\": \"sentinel-2\",\n        \"instruments\": [\"msi\"],\n        \"processing:level\": \"L2A\",\n        \"processing:software\": \"eopf-geozarr\"\n    },\n    \"geometry\": {\n        \"type\": \"Polygon\",\n        \"coordinates\": [[\n            # Extract from dataset bounds\n            [float(ds.x.min()), float(ds.y.min())],\n            [float(ds.x.max()), float(ds.y.min())],\n            [float(ds.x.max()), float(ds.y.max())],\n            [float(ds.x.min()), float(ds.y.max())],\n            [float(ds.x.min()), float(ds.y.min())]\n        ]]\n    },\n    \"assets\": {\n        \"geozarr\": {\n            \"href\": \"stac_ready.zarr\",\n            \"type\": \"application/vnd+zarr\",\n            \"roles\": [\"data\"],\n            \"title\": \"GeoZarr Dataset\"\n        }\n    }\n}\n\n# Save STAC item\nwith open(\"stac_item.json\", \"w\") as f:\n    json.dump(stac_item, f, indent=2)\n</code></pre>"},{"location":"examples/#jupyter-notebook-integration","title":"Jupyter Notebook Integration","text":"<p>Interactive exploration in Jupyter:</p> <pre><code># Cell 1: Setup and conversion\nimport xarray as xr\nimport matplotlib.pyplot as plt\nfrom eopf_geozarr import create_geozarr_dataset\n\ndt = xr.open_datatree(\"input.zarr\", engine=\"zarr\")\ndt_geozarr = create_geozarr_dataset(\n    dt_input=dt,\n    groups=[\"/measurements/r10m\"],\n    output_path=\"notebook_example.zarr\"\n)\n\n# Cell 2: Interactive visualization\n%matplotlib widget\nimport ipywidgets as widgets\n\ndef plot_band(band_name, level):\n    ds = dt_geozarr[f\"/measurements/r10m/{level}\"].ds\n    band_data = ds[band_name]\n\n    plt.figure(figsize=(10, 8))\n    band_data.plot(robust=True, cmap='viridis')\n    plt.title(f\"{band_name} - Level {level}\")\n    plt.show()\n\n# Create interactive widgets\nband_widget = widgets.Dropdown(\n    options=['b02', 'b03', 'b04', 'b08'],\n    value='b04',\n    description='Band:'\n)\n\nlevel_widget = widgets.Dropdown(\n    options=['0', '1', '2'],\n    value='0',\n    description='Level:'\n)\n\nwidgets.interact(plot_band, band_name=band_widget, level=level_widget)\n</code></pre> <p>These examples demonstrate the flexibility and power of the EOPF GeoZarr library across various use cases, from simple conversions to complex cloud-based workflows.</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":"<p>Common questions and solutions for using the EOPF GeoZarr library.</p>"},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#what-is-eopf-geozarr","title":"What is EOPF GeoZarr?","text":"<p>EOPF GeoZarr is a Python library that converts EOPF (Earth Observation Processing Framework) datasets to GeoZarr-spec 0.4 compliant format. It maintains scientific accuracy while optimizing for cloud-native workflows and performance.</p>"},{"location":"faq/#what-makes-this-different-from-standard-zarr","title":"What makes this different from standard Zarr?","text":"<p>GeoZarr is a specification that extends Zarr with geospatial metadata standards. Our library specifically:</p> <ul> <li>Ensures GeoZarr 0.4 specification compliance</li> <li>Preserves native coordinate reference systems</li> <li>Creates multiscale pyramids for efficient visualization</li> <li>Maintains CF conventions for scientific interoperability</li> <li>Optimizes chunking for Earth observation data patterns</li> </ul>"},{"location":"faq/#which-satellite-missions-are-supported","title":"Which satellite missions are supported?","text":"<p>Currently, the library is optimized for:</p> <ul> <li>Sentinel-2 (L1C and L2A products)</li> <li>Sentinel-1 (planned support)</li> </ul> <p>The architecture is designed to support additional missions with minimal modifications.</p>"},{"location":"faq/#installation-and-setup","title":"Installation and Setup","text":""},{"location":"faq/#why-do-i-need-python-311-or-higher","title":"Why do I need Python 3.11 or higher?","text":"<p>The library uses modern Python features and depends on recent versions of scientific libraries (xarray, zarr, dask) that require Python 3.11+.</p>"},{"location":"faq/#can-i-use-conda-instead-of-pip","title":"Can I use conda instead of pip?","text":"<p>While the library is primarily distributed via PyPI, you can install it in a conda environment:</p> <pre><code>conda create -n eopf-geozarr python=3.11\nconda activate eopf-geozarr\npip install eopf-geozarr\n</code></pre>"},{"location":"faq/#how-do-i-set-up-aws-credentials","title":"How do I set up AWS credentials?","text":"<p>Multiple options are available:</p> <pre><code># Option 1: AWS CLI\naws configure\n\n# Option 2: Environment variables\nexport AWS_ACCESS_KEY_ID=your_key\nexport AWS_SECRET_ACCESS_KEY=your_secret\nexport AWS_DEFAULT_REGION=us-east-1\n\n# Option 3: IAM roles (for EC2/ECS)\n# No setup needed - automatic detection\n</code></pre>"},{"location":"faq/#usage-questions","title":"Usage Questions","text":""},{"location":"faq/#how-do-i-know-which-groups-to-convert","title":"How do I know which groups to convert?","text":"<p>Inspect your EOPF dataset first:</p> <pre><code># test: skip\nimport xarray as xr\n\ndt = xr.open_datatree(\"input.zarr\", engine=\"zarr\")\nprint(dt)  # Shows the full structure\n\n# Common Sentinel-2 groups:\ngroups = [\n    \"/measurements/r10m\",  # 10m bands: B02, B03, B04, B08\n    \"/measurements/r20m\",  # 20m bands: B05, B06, B07, B8A, B11, B12\n    \"/measurements/r60m\"   # 60m bands: B01, B09, B10\n]\n</code></pre>"},{"location":"faq/#what-chunk-size-should-i-use","title":"What chunk size should I use?","text":"<p>The optimal chunk size depends on your data and use case:</p> <pre><code># test: skip\nfrom eopf_geozarr.conversion.utils import calculate_aligned_chunk_size\n\n# For typical Sentinel-2 data (10980x10980)\noptimal_chunk = calculate_aligned_chunk_size(10980, 4096)\nprint(optimal_chunk)  # Returns 3660\n\n# General guidelines:\n# - 4096: Good default for most cases\n# - 2048: Better for memory-constrained environments\n# - 8192: For high-memory systems and large datasets\n</code></pre>"},{"location":"faq/#how-do-i-process-very-large-datasets","title":"How do I process very large datasets?","text":"<p>Use Dask for distributed processing:</p> <pre><code># test: skip\nfrom dask.distributed import Client\n\n# Start Dask cluster\nclient = Client('scheduler-address:8786')\n\n# Use smaller chunks for distributed processing\ndt_geozarr = create_geozarr_dataset(\n    dt_input=dt,\n    groups=[\"/measurements/r10m\"],\n    output_path=\"output.zarr\",\n    spatial_chunk=2048  # Smaller chunks work better with Dask\n)\n</code></pre>"},{"location":"faq/#can-i-convert-only-specific-bands","title":"Can I convert only specific bands?","text":"<p>Currently, the library processes all bands within a group. To process specific bands, you would need to create a subset of your input dataset first:</p> <pre><code># test: skip\n# Create subset with specific bands\ndt_subset = dt.copy()\nds_10m = dt_subset[\"/measurements/r10m\"].ds\nds_subset = ds_10m[[\"b02\", \"b03\", \"b04\"]]  # Only RGB bands\ndt_subset[\"/measurements/r10m\"].ds = ds_subset\n\n# Then convert\ndt_geozarr = create_geozarr_dataset(\n    dt_input=dt_subset,\n    groups=[\"/measurements/r10m\"],\n    output_path=\"rgb_only.zarr\"\n)\n</code></pre>"},{"location":"faq/#error-troubleshooting","title":"Error Troubleshooting","text":""},{"location":"faq/#importerror-no-module-named-eopf_geozarr","title":"\"ImportError: No module named 'eopf_geozarr'\"","text":"<p>Cause: Library not installed or wrong Python environment.</p> <p>Solutions:</p> <pre><code># test: skip\n# Verify installation\npip list | grep eopf-geozarr\n\n# Reinstall if missing\npip install eopf-geozarr\n\n# Check Python environment\nwhich python\npython --version\n</code></pre>"},{"location":"faq/#valueerror-invalid-groups-specified","title":"\"ValueError: Invalid groups specified\"","text":"<p>Cause: Specified groups don't exist in the input dataset.</p> <p>Solution:</p> <pre><code># test: skip\n# Check available groups\ndt = xr.open_datatree(\"input.zarr\", engine=\"zarr\")\nprint(\"Available groups:\", list(dt.groups))\n\n# Use correct group paths\ngroups = [g for g in dt.groups if \"measurements\" in g]\n</code></pre>"},{"location":"faq/#memoryerror-during-conversion","title":"\"MemoryError\" during conversion","text":"<p>Cause: Dataset too large for available memory.</p> <p>Solutions:</p> <pre><code># test: skip\n# 1. Use smaller chunks\ndt_geozarr = create_geozarr_dataset(\n    dt_input=dt,\n    groups=[\"/measurements/r10m\"],\n    output_path=\"output.zarr\",\n    spatial_chunk=1024  # Smaller chunks\n)\n\n# 2. Use Dask for out-of-core processing\nfrom dask.distributed import Client\nclient = Client()\n\n# 3. Process groups one at a time\nfor group in [\"/measurements/r10m\", \"/measurements/r20m\"]:\n    dt_geozarr = create_geozarr_dataset(\n        dt_input=dt,\n        groups=[group],\n        output_path=f\"output_{group.split('/')[-1]}.zarr\"\n    )\n</code></pre>"},{"location":"faq/#permissionerror-with-s3","title":"\"PermissionError\" with S3","text":"<p>Cause: Insufficient S3 permissions or incorrect credentials.</p> <p>Solutions:</p> <pre><code># test: skip\n# 1. Verify credentials\nfrom eopf_geozarr.conversion.fs_utils import get_s3_credentials_info\nprint(get_s3_credentials_info())\n\n# 2. Test S3 access\nfrom eopf_geozarr.conversion.fs_utils import validate_s3_access\nis_valid, error = validate_s3_access(\"s3://your-bucket/test.zarr\")\nprint(f\"Valid: {is_valid}, Error: {error}\")\n\n# 3. Check IAM permissions (need s3:GetObject, s3:PutObject, s3:ListBucket)\n</code></pre>"},{"location":"faq/#zarrerrorsarraynotfounderror","title":"\"zarr.errors.ArrayNotFoundError\"","text":"<p>Cause: Corrupted or incomplete Zarr dataset.</p> <p>Solutions:</p> <pre><code># test: skip\n# 1. Validate input dataset\ntry:\n    dt = xr.open_datatree(\"input.zarr\", engine=\"zarr\")\n    print(\"Dataset loaded successfully\")\nexcept Exception as e:\n    print(f\"Dataset error: {e}\")\n\n# 2. Check for missing arrays\nimport zarr\nstore = zarr.open(\"input.zarr\", mode=\"r\")\nprint(\"Available arrays:\", list(store.array_keys()))\n\n# 3. Consolidate metadata if needed\nzarr.consolidate_metadata(\"input.zarr\")\n</code></pre>"},{"location":"faq/#crs-not-found-or-projection-errors","title":"\"CRS not found\" or projection errors","text":"<p>Cause: Missing or invalid coordinate reference system information.</p> <p>Solutions:</p> <pre><code># test: skip\n# Check CRS information\nds = dt[\"/measurements/r10m\"].ds\nprint(\"CRS variables:\", [v for v in ds.data_vars if 'crs' in v.lower() or 'spatial_ref' in v])\n\n# Check coordinate attributes\nprint(\"X coord attrs:\", ds.x.attrs)\nprint(\"Y coord attrs:\", ds.y.attrs)\n\n# Verify rioxarray can read CRS\nimport rioxarray\ntry:\n    crs = ds.rio.crs\n    print(f\"CRS: {crs}\")\nexcept Exception as e:\n    print(f\"CRS error: {e}\")\n</code></pre>"},{"location":"faq/#performance-questions","title":"Performance Questions","text":""},{"location":"faq/#why-is-conversion-slow","title":"Why is conversion slow?","text":"<p>Several factors affect performance:</p> <ol> <li>Chunk size: Too small = many operations, too large = memory issues</li> <li>Network: S3 operations depend on bandwidth and latency</li> <li>CPU: Overview generation is CPU-intensive</li> <li>Memory: Insufficient RAM causes swapping</li> </ol> <p>Optimization strategies:</p> <pre><code># test: skip\n# 1. Optimal chunking\nchunk_size = calculate_aligned_chunk_size(data_width, 4096)\n\n# 2. Use Dask for parallelization\nfrom dask.distributed import Client\nclient = Client(n_workers=4, threads_per_worker=2)\n\n# 3. Process in batches\nfor group in groups:\n    # Process one group at a time\n    pass\n\n# 4. Use SSD storage for temporary files\nimport tempfile\nimport os\nos.environ['TMPDIR'] = '/path/to/fast/storage'\n</code></pre>"},{"location":"faq/#how-can-i-monitor-progress","title":"How can I monitor progress?","text":"<p>Enable verbose logging:</p> <pre><code># test: skip\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\n# Or use the CLI with verbose flag\n# eopf-geozarr convert input.zarr output.zarr --verbose\n</code></pre>"},{"location":"faq/#whats-the-expected-output-size","title":"What's the expected output size?","text":"<p>GeoZarr datasets are typically larger than input due to:</p> <ul> <li>Multiscale overviews (adds ~33% for 2 overview levels)</li> <li>Additional metadata</li> <li>Chunk alignment padding</li> </ul> <p>Estimation:</p> <pre><code># test: skip\n# Rough estimate: input_size * 1.4 (with 2 overview levels)\n# For Sentinel-2 10m band: ~400MB input \u2192 ~560MB GeoZarr\n</code></pre>"},{"location":"faq/#cloud-storage-questions","title":"Cloud Storage Questions","text":""},{"location":"faq/#which-cloud-providers-are-supported","title":"Which cloud providers are supported?","text":"<ul> <li>AWS S3: Full support</li> <li>S3-compatible: MinIO, DigitalOcean Spaces, etc.</li> <li>Google Cloud Storage: Via S3 compatibility mode</li> <li>Azure Blob Storage: Via S3 compatibility (limited)</li> </ul>"},{"location":"faq/#how-do-i-optimize-s3-performance","title":"How do I optimize S3 performance?","text":"<pre><code># test: skip\n# 1. Use appropriate region\nos.environ['AWS_DEFAULT_REGION'] = 'us-west-2'  # Close to your data\n\n# 2. Configure multipart uploads\ns3_config = {\n    'config_kwargs': {\n        'max_pool_connections': 50,\n        'multipart_threshold': 64 * 1024 * 1024,  # 64MB\n        'multipart_chunksize': 16 * 1024 * 1024   # 16MB\n    }\n}\n\n# 3. Use VPC endpoints for EC2 instances\n# 4. Consider S3 Transfer Acceleration for global access\n</code></pre>"},{"location":"faq/#can-i-use-different-storage-for-input-and-output","title":"Can I use different storage for input and output?","text":"<p>Yes, the library supports mixed storage:</p> <pre><code># test: skip\n# Local input, S3 output\ndt = xr.open_datatree(\"local_input.zarr\", engine=\"zarr\")\ndt_geozarr = create_geozarr_dataset(\n    dt_input=dt,\n    groups=[\"/measurements/r10m\"],\n    output_path=\"s3://bucket/output.zarr\"\n)\n\n# S3 input, local output\ndt = xr.open_datatree(\"s3://bucket/input.zarr\", engine=\"zarr\")\ndt_geozarr = create_geozarr_dataset(\n    dt_input=dt,\n    groups=[\"/measurements/r10m\"],\n    output_path=\"local_output.zarr\"\n)\n</code></pre>"},{"location":"faq/#validation-and-quality","title":"Validation and Quality","text":""},{"location":"faq/#how-do-i-verify-the-conversion-worked-correctly","title":"How do I verify the conversion worked correctly?","text":"<pre><code># test: skip\n# 1. Use built-in validation\nfrom eopf_geozarr.cli import validate_command\nimport argparse\n\nargs = argparse.Namespace()\nargs.input_path = \"output.zarr\"\nargs.verbose = True\nvalidate_command(args)\n\n# 2. Manual checks\ndt = xr.open_datatree(\"output.zarr\", engine=\"zarr\")\n\n# Check multiscales metadata\nprint(\"Multiscales:\", dt.attrs.get('multiscales', 'Missing'))\n\n# Check overview levels\nfor level in ['0', '1', '2']:\n    path = f\"/measurements/r10m/{level}\"\n    if path in dt.groups:\n        ds = dt[path].ds\n        print(f\"Level {level}: {dict(ds.dims)}\")\n\n# Check required attributes\nds = dt[\"/measurements/r10m/0\"].ds\nfor var_name in ds.data_vars:\n    var = ds[var_name]\n    print(f\"{var_name}: grid_mapping={var.attrs.get('grid_mapping', 'Missing')}\")\n</code></pre>"},{"location":"faq/#what-should-i-do-if-validation-fails","title":"What should I do if validation fails?","text":"<ol> <li>Check the error message - it usually indicates the specific issue</li> <li>Verify input data - ensure the EOPF dataset is complete and valid</li> <li>Check dependencies - ensure all required libraries are up to date</li> <li>Try with verbose logging - get more detailed error information</li> <li>Report issues - if it seems like a bug, please report it</li> </ol>"},{"location":"faq/#how-do-i-compare-input-and-output-data","title":"How do I compare input and output data?","text":"<pre><code># test: skip\n# Load both datasets\ndt_input = xr.open_datatree(\"input.zarr\", engine=\"zarr\")\ndt_output = xr.open_datatree(\"output.zarr\", engine=\"zarr\")\n\n# Compare native resolution data\nds_input = dt_input[\"/measurements/r10m\"].ds\nds_output = dt_output[\"/measurements/r10m/0\"].ds\n\n# Check data values (should be identical)\nimport numpy as np\nfor band in [\"b02\", \"b03\", \"b04\"]:\n    if band in ds_input and band in ds_output:\n        diff = np.abs(ds_input[band].values - ds_output[band].values)\n        print(f\"{band} max difference: {diff.max()}\")\n        # Should be 0 or very close to 0\n</code></pre>"},{"location":"faq/#integration-questions","title":"Integration Questions","text":""},{"location":"faq/#can-i-use-this-with-stac","title":"Can I use this with STAC?","text":"<p>Yes, you can create STAC items for GeoZarr datasets. See the Examples section for detailed code.</p>"},{"location":"faq/#how-does-this-work-with-jupyter-notebooks","title":"How does this work with Jupyter notebooks?","text":"<p>The library works well in Jupyter environments. See Examples for interactive visualization patterns.</p>"},{"location":"faq/#can-i-integrate-this-into-my-processing-pipeline","title":"Can I integrate this into my processing pipeline?","text":"<p>Absolutely! The library is designed for integration:</p> <pre><code># test: skip\n# Example pipeline integration\ndef process_sentinel2_scene(input_path: str, output_path: str):\n    \"\"\"Process a single Sentinel-2 scene to GeoZarr.\"\"\"\n    try:\n        dt = xr.open_datatree(input_path, engine=\"zarr\")\n        dt_geozarr = create_geozarr_dataset(\n            dt_input=dt,\n            groups=[\"/measurements/r10m\", \"/measurements/r20m\"],\n            output_path=output_path,\n            spatial_chunk=4096\n        )\n        return True, \"Success\"\n    except Exception as e:\n        return False, str(e)\n\n# Use in batch processing\nresults = []\nfor scene in scene_list:\n    success, message = process_sentinel2_scene(scene.input, scene.output)\n    results.append((scene.id, success, message))\n</code></pre>"},{"location":"faq/#getting-help","title":"Getting Help","text":""},{"location":"faq/#where-can-i-get-more-help","title":"Where can I get more help?","text":"<ol> <li>Documentation: Check the User Guide and API Reference</li> <li>Examples: See Examples for common use cases</li> <li>GitHub Issues: Report bugs or request features at the GitHub repository</li> <li>Community: Join discussions in the GeoZarr community</li> </ol>"},{"location":"faq/#how-do-i-report-a-bug","title":"How do I report a bug?","text":"<p>When reporting issues, please include:</p> <ol> <li>Version information:</li> </ol> <pre><code># test: skip\neopf-geozarr --version\npython --version\npip list | grep -E \"(xarray|zarr|dask)\"\n</code></pre> <ol> <li> <p>Error message: Full traceback if available</p> </li> <li> <p>Minimal example: Code that reproduces the issue</p> </li> <li> <p>Environment: OS, Python version, installation method</p> </li> <li> <p>Data information: Dataset type, size, structure (if shareable)</p> </li> </ol>"},{"location":"faq/#how-can-i-contribute","title":"How can I contribute?","text":"<p>Contributions are welcome! See the project repository for contribution guidelines. Areas where help is needed:</p> <ul> <li>Additional satellite mission support</li> <li>Performance optimizations</li> <li>Documentation improvements</li> <li>Test coverage expansion</li> <li>Bug fixes and feature enhancements</li> </ul>"},{"location":"geozarr-minispec/","title":"GeoZarr Mini Spec","text":"<p>This document specifies the GeoZarr model used in this repository. It's a \"mini\" version of the official GeoZarr spec that documents the specific subset of the specification that this implementation supports, along with implementation-specific details.</p>"},{"location":"geozarr-minispec/#relationship-to-other-documentation","title":"Relationship to Other Documentation","text":"<p>This mini spec is referenced by and aligns with:</p> <ul> <li>Architecture - Technical implementation details that follow this specification</li> <li>GeoZarr Specification Contribution - Our contributions to the official spec based on this implementation</li> <li>Main Documentation - General library documentation and usage guides</li> </ul> <p>The implementation described in this mini spec addresses specific requirements for Earth observation data processing while maintaining compliance with the broader GeoZarr specification.</p>"},{"location":"geozarr-minispec/#spec-conventions","title":"Spec conventions","text":""},{"location":"geozarr-minispec/#array-and-group-attributes","title":"Array and Group attributes","text":"<p>This document only defines rules for a finite subset of the keys in Zarr array  and group attributes. Unless otherwise stated, any external keys in Zarr array and group attributes are consistent with this specification. This means this specification composes with the presence of, e.g., CF metadata, at different levels of the Zarr hierarchy.</p>"},{"location":"geozarr-minispec/#organization","title":"Organization","text":"<p>GeoZarr defines a Zarr hierarchy, i.e. a particular arrangements of Zarr arrays and groups, and  their attributes. This document defines that hierarchy from the bottom-up, starting with arrays and their attributes before moving to higher-level structures, like groups and their attributes.</p> <p>The GeoZarr specification can be implemented in Zarr V2 and V3. The main difference between the Zarr V2 and Zarr V3 implementations is how the dimension names of an array are specified.</p>"},{"location":"geozarr-minispec/#dataarray","title":"DataArray","text":"<p>A DataArray is a Zarr array with named axes. The structure of a DataArray depends on the Zarr format.</p> <p>This section contains the rules for individual DataArrays. Additional  constraints on groups of DataArrays are defined in the section on Datasets</p>"},{"location":"geozarr-minispec/#zarr-v2","title":"Zarr V2","text":""},{"location":"geozarr-minispec/#attributes","title":"Attributes","text":"key type required notes _ARRAY_DIMENSIONS array of strings, length matches number of axes of the array yes xarray convention for naming axes in Zarr V2"},{"location":"geozarr-minispec/#array-metadata","title":"Array metadata","text":"<p>Zarr V2 DataArrays must have at least 1 dimension, i.e. scalar Zarr V2 DataArrays are not allowed.</p> <p>In tabular form: </p> attribute constraint notes <code>shape</code> at least 1 element No scalar arrays allowed"},{"location":"geozarr-minispec/#example","title":"Example","text":"<pre><code>{\n    \".zarray\": {\n        \"zarr_format\": 2,\n        \"dtype\": \"|u1\",\n        \"shape\": [10,11,12],\n        \"chunks\": [10,11,12],\n        \"filters\": null\n        \"compressor\": null\n        \"order\": \"C\"\n        \"dimension_separator\": \"/\"\n        }\n    \".zattrs\": {\n        \"_ARRAY_DIMENSIONS\": [\"lat\", \"lon\", \"time\"]\n        }\n\n}\n</code></pre>"},{"location":"geozarr-minispec/#zarr-v3","title":"Zarr V3","text":""},{"location":"geozarr-minispec/#attributes_1","title":"Attributes","text":"<p>No particular attributes are required for Zarr V3 DataArrays.</p>"},{"location":"geozarr-minispec/#array-metadata_1","title":"Array metadata","text":"<p>Zarr V3 DataArrays must have at least 1 dimension, i.e. scalar Zarr V3 DataArrays are not allowed. The  <code>dimension_names</code> attribute of a Zarr V3 DataArray must be set, the elements of <code>dimension_names</code> must  all be strings, and they must all be unique.</p> <p>In tabular form:</p> attribute constraint notes <code>shape</code> at least 1 element No scalar arrays allowed <code>dimension_names</code> an array of unique strings all array axes must be uniquely named."},{"location":"geozarr-minispec/#example_1","title":"Example","text":"<pre><code>{\n    \"zarr.json\": {\n        \"zarr_format\": 3,\n        \"node_type\": \"array\",\n        \"shape\": [10,11,12],\n        \"data_type\": \"uint8\",\n        \"chunk_key_encoding\": {\"name\": \"default\", \"configuration\": {\"separator\" : \"/\"}},\n        \"chunk_grid\": {\"name\": \"regular\", \"configuration\": {\"chunk_shape\": [10,11,12]}},\n        \"codecs\": [{\"name\": \"bytes\"}],\n        \"dimension_names\": [\"lat\", \"lon\", \"time\"],\n        \"storage_transformers\": [],\n        }\n}\n</code></pre>"},{"location":"geozarr-minispec/#dataset","title":"Dataset","text":"<p>A GeoZarr dataset is a Zarr group that contains Zarr arrays that together describe a measured quantity,  as well as arbitrary sub-groups.</p>"},{"location":"geozarr-minispec/#attributes_2","title":"Attributes","text":"<p>There are no required attributes for Datasets but to qualify as a GeoZarr Dataset, the group must contain at least one DataArray with spatial reference information. This DataArray is referenced in the <code>grid_mapping</code> attribute of the dataset and is usually named <code>spatial_ref</code>.</p>"},{"location":"geozarr-minispec/#cf-compliance-requirements","title":"CF Compliance Requirements","text":"<p>The implementation enforces CF (Climate and Forecast) metadata conventions compliance:</p> <ul> <li>Grid Mapping: All data variables MUST include a <code>grid_mapping</code> attribute that references a coordinate reference system variable</li> <li>Standard Names: Data variables MUST include CF-compliant <code>standard_name</code> attributes. The implementation validates these against the official CF standard names table</li> <li>Coordinate Variables: Coordinate variables (x, y, time, etc.) MUST include appropriate CF standard names:</li> <li>For projected coordinates: <code>projection_x_coordinate</code> and <code>projection_y_coordinate</code> </li> <li>For geographic coordinates: <code>longitude</code> and <code>latitude</code></li> <li>Units must be specified (<code>m</code> for projected, <code>degrees_east</code>/<code>degrees_north</code> for geographic)</li> <li>Array Dimensions: All arrays MUST include <code>_ARRAY_DIMENSIONS</code> attributes for Zarr V2 compatibility</li> </ul> <p>More information on spatial reference information can be found in the CF conventions. Another interesting resource is the rioxarray and more specifically the documentation on Coordinate Reference System Management.</p>"},{"location":"geozarr-minispec/#members","title":"Members","text":"<p>If any member of a GeoZarr Dataset is an array, then it must comply with the DataArray definition.</p> <p>If the Dataset contains a DataArray <code>D</code>, then for each dimension name <code>N</code> in the list of <code>D</code>'s named dimensions,  the Dataset must contain a one-dimensional DataArray named <code>N</code> with a shape that matches the the length  of <code>D</code> along the axis named by <code>N</code>. In this case, <code>D</code> is called a \"data variable\", and the each  DataArrays matching a dimension names of <code>D</code> is called a \"coordinate variable\". </p> <p>[!Note] These two definitions are not mutually exclusive, as a 1-dimensional DataArray named <code>D</code> with dimension names <code>[\"D\"]</code> is both a coordinate variable and a data variable.</p>"},{"location":"geozarr-minispec/#examples","title":"Examples","text":"<p>This example demonstrates the stored representation of a valid Dataset. Notice how  the dimension names defined on the DataArray named <code>\"data\"</code> (i.e., <code>\"lat\"</code> and <code>\"lon\"</code>) are  the names of one-dimensional DataArrays in the same Zarr group as <code>\"data\"</code>.</p> <p>In this case, <code>\"data\"</code> is a data variable, and <code>\"lat\"</code> and <code>\"lon\"</code> are coordinate variables.</p> <pre><code>{\n    \"zarr.json\" : {\n        \"node_type\": \"group\",\n        \"zarr_format\": 3,\n        },\n    \"data/zarr.json\" : {\n        \"zarr_format\": 3,\n        \"node_type\": \"array\",\n        \"shape\": [10,11],\n        \"data_type\": \"uint8\",\n        \"chunk_key_encoding\": {\"name\": \"default\", \"configuration\": {\"separator\" : \"/\"}},\n        \"chunk_grid\": {\"name\": \"regular\", \"configuration\": {\"chunk_shape\": [10,11]}},\n        \"codecs\": [{\"name\": \"bytes\"}],\n        \"dimension_names\": [\"lat\", \"lon\"],\n        \"storage_transformers\": [],\n        },\n    \"lat/zarr.json\": {\n        \"zarr_format\": 3,\n        \"node_type\": \"array\",\n        \"shape\": [10],\n        \"data_type\": \"uint8\",\n        \"chunk_key_encoding\": {\"name\": \"default\", \"configuration\": {\"separator\" : \"/\"}},\n        \"chunk_grid\": {\"name\": \"regular\", \"configuration\": {\"chunk_shape\": [10]}},\n        \"codecs\": [{\"name\": \"bytes\"}],\n        \"dimension_names\": [\"lat\"],\n        \"storage_transformers\": [],\n        },\n    \"lon/zarr.json\": {\n        \"zarr_format\": 3,\n        \"node_type\": \"array\",\n        \"shape\": [11],\n        \"data_type\": \"uint8\",\n        \"chunk_key_encoding\": {\"name\": \"default\", \"configuration\": {\"separator\" : \"/\"}},\n        \"chunk_grid\": {\"name\": \"regular\", \"configuration\": {\"chunk_shape\": [11]}},\n        \"codecs\": [{\"name\": \"bytes\"}],\n        \"dimension_names\": [\"lon\"],\n        \"storage_transformers\": [],\n        },\n}\n</code></pre> <p>This example demonstrates the layout of a Dataset with just one DataArray. A single array is only permitted if that array is one dimensional, and the name of that DataArray in the Dataset  matches the (single) dimension name defined for that DataArray. </p> <p>In this case <code>lat</code> is both a coordinate variable and a data variable.</p> <pre><code>{\n    \"zarr.json\" : {\n        \"node_type\": \"group\",\n        \"zarr_format\": 3,\n        },\n    \"lat/zarr.json\" : {\n        \"zarr_format\": 3,\n        \"node_type\": \"array\",\n        \"shape\": [10],\n        \"data_type\": \"uint8\",\n        \"chunk_key_encoding\": {\"name\": \"default\", \"configuration\": {\"separator\" : \"/\"}},\n        \"chunk_grid\": {\"name\": \"regular\", \"configuration\": {\"chunk_shape\": [10,11]}},\n        \"codecs\": [{\"name\": \"bytes\"}],\n        \"dimension_names\": [\"lat\"],\n        \"storage_transformers\": [],\n    },\n}\n</code></pre>"},{"location":"geozarr-minispec/#multiscale-dataset","title":"Multiscale Dataset","text":"<p>This implementation supports two multiscales metadata conventions:</p> <ol> <li>Zarr Multiscales Convention: An experimental convention for describing multi-resolution data with simple scale and translation metadata</li> <li>GeoZarr 0.4 TileMatrixSet Specification: The experimental GeoZarr specification using OGC TileMatrixSet definitions for geospatial data</li> </ol> <p>Both conventions can coexist in the same Zarr store, providing flexibility for different use cases and ensuring compatibility with various tools and workflows. The implementation follows the specifications defined in:</p> <ul> <li>Zarr Multiscales Convention Specification - For the experimental multiscales convention with examples at https://github.com/zarr-conventions/multiscales/tree/main/examples</li> <li>See particularly sentinel-2-multiresolution.json for Sentinel-2 multi-resolution structure</li> <li>GeoZarr Specification - For the OGC TileMatrixSet-based convention</li> </ul> <p>When both conventions are enabled, they are written to the same group attributes with different keys, allowing tools to use whichever convention they support.</p> <p>Downsampling is a process in which a collection of localized data points is resampled on a subset of its original sampling locations. </p> <p>In the case of arrays, downsampling generally reduces an array's shape along at least one dimension. To downsample the contents of a Dataset <code>D</code> and generate a new Dataset <code>E</code>, all of the coordinate variable - data variable  relationships in <code>D</code> must be preserved in <code>E</code>. If <code>D/data</code> is a data variable with dimension names (<code>\"a\"</code> , <code>\"b\"</code>), then <code>D/a</code> and <code>D/b</code> are coordinate variables with shapes aligned to the dimensions of <code>D/data</code>. If we downsample <code>D/data</code> and assign the result to <code>E/data</code>, we must also generate (e.g., by more downsampling) coordinate variables <code>E/a</code> and <code>E/b</code> so that <code>E</code> can be a valid Dataset according to the relevant Dataset members rule.</p> <p>The downsampling transformation is thus well-defined for Datasets. Downsampling  is often applied multiple times in a series, e.g. to generate multiple levels of  detail for a data variable. </p>"},{"location":"geozarr-minispec/#implementation-approach","title":"Implementation Approach","text":"<p>The implementation uses a pyramid-based downsampling approach with the following characteristics:</p> <ul> <li>Variable Downsampling Factors: Overview levels use optimal downsampling factors based on data characteristics (e.g., 2x, 3x) rather than strictly factor-of-2. For Sentinel-2, this results in resolution levels: 10m \u2192 20m \u2192 60m \u2192 120m (2x) \u2192 360m (3x) \u2192 720m (2x)</li> <li>Pyramid Generation: Overview levels are created sequentially, with each level generated from the previous level rather than from the native resolution</li> <li>Minimum Dimension Threshold: Overview generation stops when the smallest dimension falls below a configurable threshold (default: 256 pixels)</li> <li>Native CRS Preservation: All overview levels maintain the same coordinate reference system as the native data</li> <li>Consistent Variable Structure: Each overview level contains the same set of variables as the native resolution level</li> </ul> <p>GeoZarr defines a layout for downsampled Datasets (and the original dataset). Given some source Dataset <code>s0</code>,  that dataset and all downsampled Datasets <code>s1</code>, <code>s2</code>, ... are stored in a flat layout inside a Multiscale Dataset  <code>D</code>. The presence of downsampled Datsets in <code>D</code> is signalled by a special key in the attributes of <code>D</code>.</p>"},{"location":"geozarr-minispec/#attributes_3","title":"Attributes","text":"<p>The attributes of a Multiscale Dataset function as an entry point to a collection of downsampled Datasets. Accordingly, the attributes of a Multiscale Dataset declare the names of the downsampled datasets it contains, as well as spatial metadata for those datasets.</p> key type required notes <code>\"multiscales\"</code> <code>MultiscaleMetadata</code> yes this field defines the layout of the multiscale Datasets inside this Dataset"},{"location":"geozarr-minispec/#multiscalemetadata","title":"MultiscaleMetadata","text":"<p><code>MultiscaleMetadata</code> is a JSON object that declares the names of the downsampled Datasets inside a Multiscale Dataset, as well as the downsampling method used. This object has the following structure:</p> key type required notes <code>\"resampling_method\"</code> ResamplingMethod yes This is a string that declares the resampling method used to create the downsampled datasets. <code>\"tile_matrix_set\"</code> TileMatrixSet or string yes This object declares the names of the downsampled Datasets. If <code>\"tile_matrix_set\"</code> is a string, it must be the name of a well-known <code>TileMatrixSet</code>, which must resolve to a JSON object consistent with the <code>[TileMatrixSet](#tilematrixset)</code> definition. For scientific coordinate systems, custom inline TileMatrixSet objects are supported. <code>\"tile_matrix_limits\"</code> {<code>string</code>: TileMatrixLimit} no Optional limits for each tile matrix level"},{"location":"geozarr-minispec/#members_1","title":"Members","text":"<p>All of the members declared in the <code>multiscales</code> attribute must comply with the Dataset definition. All of these Datasets must have the exact same set of member names. The names of the downsampled Datasets are specified by  the <code>\"id\"</code> field of each <code>TileMatrix</code> object in the <code>\"tileMatrices\"</code> field in the <code>\"TileMatrixSet\"</code> object in the <code>tile_matrix_set</code> field in the <code>MultiscaleMetadata</code> object in the <code>\"multiscales\"</code> field in the attributes of the Multiscale Dataset. Or, more compactly, using a path-like JSON query:</p> <p><code>attributes.multiscales.tile_matrix_set.tileMatrices[$idx].id</code></p>"},{"location":"geozarr-minispec/#chunking-requirements-for-downsampled-datasets","title":"Chunking Requirements for Downsampled Datasets","text":"<p>When creating downsampled datasets in a multiscale hierarchy, careful consideration must be given to chunk sizes to ensure optimal performance and storage efficiency. The chunk dimensions should be aligned with the tile dimensions specified in the corresponding <code>TileMatrix</code> definition to enable efficient tile-based access patterns.</p> <p>Key chunking considerations:</p> <ul> <li>Chunk-Tile Alignment: Chunk sizes should match or be divisible by the <code>tileWidth</code> and <code>tileHeight</code> values defined in the <code>TileMatrix</code> for each zoom level</li> <li>Consistent Chunking Strategy: All data variables within a zoom level should use the same chunking scheme to maintain spatial coherence</li> <li>Memory Constraints: Chunk sizes should be chosen to balance I/O efficiency with memory usage, typically keeping individual chunks under 100MB</li> <li>Decimation Factor Alignment: When downsampling by integer factors (e.g., 2x, 3x), chunk boundaries should align across zoom levels to enable efficient pyramid generation</li> </ul> <p>For example, if a <code>TileMatrix</code> specifies <code>tileWidth: 1024</code> and <code>tileHeight: 1024</code>, the corresponding data arrays should use chunk shapes of <code>[1024, 1024]</code> or compatible subdivisions like <code>[512, 512]</code>.</p>"},{"location":"geozarr-minispec/#extra-members","title":"Extra members","text":"<p>A multiscale Dataset should not contain any members that are not explicitly declared in the <code>\"multiscales\"</code> field for that multiscale Dataset. Any additional Zarr arrays and groups should be considered external to the GeoZarr model.  </p>"},{"location":"geozarr-minispec/#custom-coordinate-reference-systems","title":"Custom Coordinate Reference Systems","text":"<p>GeoZarr explicitly supports custom TileMatrixSet definitions for arbitrary coordinate reference systems, encouraging preservation of native CRS in Earth observation data. This is particularly useful for scientific projections including UTM zones, polar stereographic, sinusoidal, and other non-web coordinate systems.</p> <p>For a dataset to be GeoZarr compliant, data variables MUST include a <code>grid_mapping</code> attribute that references a coordinate reference system variable. This <code>grid_mapping</code> variable defines the spatial referencing information and MUST be consistent with the CRS specified in the TileMatrixSet.</p>"},{"location":"geozarr-minispec/#custom-tilematrixset-example","title":"Custom TileMatrixSet Example","text":"<p>For custom coordinate systems, the <code>tile_matrix_set</code> should be defined as an inline JSON object following the OGC TileMatrixSet v2.0 specification:</p> <pre><code>{\n  \"multiscales\": {\n    \"tile_matrix_set\": {\n      \"id\": \"UTM_Zone_33N_Custom\",\n      \"title\": \"UTM Zone 33N for Sentinel-2 native resolution\",\n      \"crs\": \"EPSG:32633\", \n      \"orderedAxes\": [\"E\", \"N\"],\n      \"tileMatrices\": [\n        {\n          \"id\": \"0\",\n          \"scaleDenominator\": 35.28,\n          \"cellSize\": 10.0,\n          \"pointOfOrigin\": [299960.0, 9000000.0],\n          \"tileWidth\": 1024,\n          \"tileHeight\": 1024,\n          \"matrixWidth\": 1094,\n          \"matrixHeight\": 1094\n        },\n        {\n          \"id\": \"1\", \n          \"scaleDenominator\": 70.56,\n          \"cellSize\": 20.0,\n          \"pointOfOrigin\": [299960.0, 9000000.0],\n          \"tileWidth\": 512,\n          \"tileHeight\": 512,\n          \"matrixWidth\": 547,\n          \"matrixHeight\": 547\n        }\n      ]\n    },\n    \"resampling_method\": \"average\"\n  }\n}\n</code></pre>"},{"location":"geozarr-minispec/#custom-decimation-factors","title":"Custom Decimation Factors","text":"<p>While standard web mapping assumes quadtree decimation (scaling by factor of 2), custom TileMatrixSets may use alternative decimation factors:</p> <ul> <li>Factor of 2 (quadtree): Standard web mapping approach where each zoom level has 4x more tiles</li> <li>Factor of 3 (nonary tree): Each zoom level has 9x more tiles, useful for certain scientific gridding schemes  </li> <li>Other integer factors: Application-specific requirements may dictate alternative decimation</li> </ul> <p>Example with factor-of-3 decimation:</p> <pre><code>{\n  \"id\": \"Custom_Nonary_Grid\",\n  \"crs\": \"EPSG:4326\",\n  \"tileMatrices\": [\n    {\n      \"id\": \"0\",\n      \"matrixWidth\": 1,\n      \"matrixHeight\": 1,\n      \"tileWidth\": 256,\n      \"tileHeight\": 256\n    },\n    {\n      \"id\": \"1\", \n      \"matrixWidth\": 3,\n      \"matrixHeight\": 3,\n      \"tileWidth\": 256,\n      \"tileHeight\": 256\n    },\n    {\n      \"id\": \"2\",\n      \"matrixWidth\": 9,\n      \"matrixHeight\": 9,\n      \"tileWidth\": 256,\n      \"tileHeight\": 256\n    }\n  ]\n}\n</code></pre>"},{"location":"geozarr-minispec/#custom-crs-multiscale-dataset-layout-example","title":"Custom CRS Multiscale Dataset Layout Example","text":"<p>Here's a complete example of a multiscale dataset using a custom UTM coordinate reference system:</p> <pre><code>{\n  \"zarr.json\": {\n    \"node_type\": \"group\",\n    \"zarr_format\": 3,\n    \"attributes\": {\n      \"multiscales\": {\n        \"tile_matrix_set\": {\n          \"id\": \"UTM_Zone_33N_Sentinel2\",\n          \"title\": \"UTM Zone 33N for Sentinel-2 L2A\",\n          \"crs\": \"EPSG:32633\",\n          \"orderedAxes\": [\"E\", \"N\"],\n          \"tileMatrices\": [\n            {\n              \"id\": \"0\",\n              \"scaleDenominator\": 35.28,\n              \"cellSize\": 10.0,\n              \"pointOfOrigin\": [299960.0, 9000000.0],\n              \"tileWidth\": 1024,\n              \"tileHeight\": 1024,\n              \"matrixWidth\": 1094,\n              \"matrixHeight\": 1094\n            },\n            {\n              \"id\": \"1\",\n              \"scaleDenominator\": 70.56,\n              \"cellSize\": 20.0,\n              \"pointOfOrigin\": [299960.0, 9000000.0],\n              \"tileWidth\": 512,\n              \"tileHeight\": 512,\n              \"matrixWidth\": 547,\n              \"matrixHeight\": 547\n            }\n          ]\n        },\n        \"resampling_method\": \"average\"\n      }\n    }\n  },\n  \"0/zarr.json\": {\n    \"node_type\": \"group\",\n    \"zarr_format\": 3\n  },\n  \"0/red/zarr.json\": {\n    \"zarr_format\": 3,\n    \"node_type\": \"array\",\n    \"shape\": [1094, 1094],\n    \"data_type\": \"uint16\",\n    \"chunk_grid\": {\"name\": \"regular\", \"configuration\": {\"chunk_shape\": [1024, 1024]}},\n    \"codecs\": [{\"name\": \"bytes\"}],\n    \"dimension_names\": [\"y\", \"x\"],\n    \"attributes\": {\n      \"grid_mapping\": \"spatial_ref\"\n    }\n  },\n  \"0/nir/zarr.json\": {\n    \"zarr_format\": 3,\n    \"node_type\": \"array\", \n    \"shape\": [1094, 1094],\n    \"data_type\": \"uint16\",\n    \"chunk_grid\": {\"name\": \"regular\", \"configuration\": {\"chunk_shape\": [1024, 1024]}},\n    \"codecs\": [{\"name\": \"bytes\"}],\n    \"dimension_names\": [\"y\", \"x\"],\n    \"attributes\": {\n      \"grid_mapping\": \"spatial_ref\"\n    }\n  },\n  \"0/spatial_ref/zarr.json\": {\n    \"zarr_format\": 3,\n    \"node_type\": \"array\",\n    \"shape\": [],\n    \"data_type\": \"int32\",\n    \"chunk_grid\": {\"name\": \"regular\", \"configuration\": {\"chunk_shape\": []}},\n    \"codecs\": [{\"name\": \"bytes\"}],\n    \"dimension_names\": [],\n    \"attributes\": {\n      \"crs_wkt\": \"PROJCS[\\\"WGS 84 / UTM zone 32N\\\",GEOGCS[\\\"WGS 84\\\",DATUM[\\\"WGS_1984\\\",SPHEROID[\\\"WGS 84\\\",6378137,298.257223563,AUTHORITY[\\\"EPSG\\\",\\\"7030\\\"]],AUTHORITY[\\\"EPSG\\\",\\\"6326\\\"]],PRIMEM[\\\"Greenwich\\\",0,AUTHORITY[\\\"EPSG\\\",\\\"8901\\\"]],UNIT[\\\"degree\\\",0.0174532925199433,AUTHORITY[\\\"EPSG\\\",\\\"9122\\\"]],AUTHORITY[\\\"EPSG\\\",\\\"4326\\\"]],PROJECTION[\\\"Transverse_Mercator\\\"],PARAMETER[\\\"latitude_of_origin\\\",0],PARAMETER[\\\"central_meridian\\\",9],PARAMETER[\\\"scale_factor\\\",0.9996],PARAMETER[\\\"false_easting\\\",500000],PARAMETER[\\\"false_northing\\\",0],UNIT[\\\"metre\\\",1,AUTHORITY[\\\"EPSG\\\",\\\"9001\\\"]],AXIS[\\\"Easting\\\",EAST],AXIS[\\\"Northing\\\",NORTH],AUTHORITY[\\\"EPSG\\\",\\\"32632\\\"]]\",\n    \"semi_major_axis\": 6378137.0,\n    \"semi_minor_axis\": 6356752.314245179,\n    \"inverse_flattening\": 298.257223563,\n    \"reference_ellipsoid_name\": \"WGS 84\",\n    \"longitude_of_prime_meridian\": 0.0,\n    \"prime_meridian_name\": \"Greenwich\",\n    \"geographic_crs_name\": \"WGS 84\",\n    \"horizontal_datum_name\": \"World Geodetic System 1984\",\n    \"projected_crs_name\": \"WGS 84 / UTM zone 32N\",\n    \"grid_mapping_name\": \"transverse_mercator\",\n    \"latitude_of_projection_origin\": 0.0,\n    \"longitude_of_central_meridian\": 9.0,\n    \"false_easting\": 500000.0,\n    \"false_northing\": 0.0,\n    \"scale_factor_at_central_meridian\": 0.9996,\n    \"spatial_ref\": \"PROJCS[\\\"WGS 84 / UTM zone 32N\\\",GEOGCS[\\\"WGS 84\\\",DATUM[\\\"WGS_1984\\\",SPHEROID[\\\"WGS 84\\\",6378137,298.257223563,AUTHORITY[\\\"EPSG\\\",\\\"7030\\\"]],AUTHORITY[\\\"EPSG\\\",\\\"6326\\\"]],PRIMEM[\\\"Greenwich\\\",0,AUTHORITY[\\\"EPSG\\\",\\\"8901\\\"]],UNIT[\\\"degree\\\",0.0174532925199433,AUTHORITY[\\\"EPSG\\\",\\\"9122\\\"]],AUTHORITY[\\\"EPSG\\\",\\\"4326\\\"]],PROJECTION[\\\"Transverse_Mercator\\\"],PARAMETER[\\\"latitude_of_origin\\\",0],PARAMETER[\\\"central_meridian\\\",9],PARAMETER[\\\"scale_factor\\\",0.9996],PARAMETER[\\\"false_easting\\\",500000],PARAMETER[\\\"false_northing\\\",0],UNIT[\\\"metre\\\",1,AUTHORITY[\\\"EPSG\\\",\\\"9001\\\"]],AXIS[\\\"Easting\\\",EAST],AXIS[\\\"Northing\\\",NORTH],AUTHORITY[\\\"EPSG\\\",\\\"32632\\\"]]\",\n    \"_ARRAY_DIMENSIONS\": [],\n    \"GeoTransform\": \"300000.0 10.0 0.0 5000040.0 0.0 -10.0\"\n    }\n  },\n  \"0/x/zarr.json\": {\n    \"zarr_format\": 3,\n    \"node_type\": \"array\",\n    \"shape\": [1094],\n    \"data_type\": \"float64\",\n    \"chunk_grid\": {\"name\": \"regular\", \"configuration\": {\"chunk_shape\": [1094]}},\n    \"codecs\": [{\"name\": \"bytes\"}],\n    \"dimension_names\": [\"x\"]\n  },\n  \"0/y/zarr.json\": {\n    \"zarr_format\": 3,\n    \"node_type\": \"array\",\n    \"shape\": [1094],\n    \"data_type\": \"float64\", \n    \"chunk_grid\": {\"name\": \"regular\", \"configuration\": {\"chunk_shape\": [1094]}},\n    \"codecs\": [{\"name\": \"bytes\"}],\n    \"dimension_names\": [\"y\"]\n  },\n  \"1/zarr.json\": {\n    \"node_type\": \"group\",\n    \"zarr_format\": 3\n  },\n  \"1/red/zarr.json\": {\n    \"zarr_format\": 3,\n    \"node_type\": \"array\",\n    \"shape\": [547, 547],\n    \"data_type\": \"uint16\",\n    \"chunk_grid\": {\"name\": \"regular\", \"configuration\": {\"chunk_shape\": [512, 512]}},\n    \"codecs\": [{\"name\": \"bytes\"}],\n    \"dimension_names\": [\"y\", \"x\"],\n    \"attributes\": {\n      \"grid_mapping\": \"spatial_ref\"\n    }\n  },\n  \"1/nir/zarr.json\": {\n    \"zarr_format\": 3,\n    \"node_type\": \"array\",\n    \"shape\": [547, 547], \n    \"data_type\": \"uint16\",\n    \"chunk_grid\": {\"name\": \"regular\", \"configuration\": {\"chunk_shape\": [512, 512]}},\n    \"codecs\": [{\"name\": \"bytes\"}],\n    \"dimension_names\": [\"y\", \"x\"],\n    \"attributes\": {\n      \"grid_mapping\": \"spatial_ref\"\n    }\n  },\n  \"1/spatial_ref/zarr.json\": {\n    \"zarr_format\": 3,\n    \"node_type\": \"array\",\n    \"shape\": [],\n    \"data_type\": \"int32\",\n    \"chunk_grid\": {\"name\": \"regular\", \"configuration\": {\"chunk_shape\": []}},\n    \"codecs\": [{\"name\": \"bytes\"}],\n    \"dimension_names\": [],\n    \"attributes\": {\n      \"crs_wkt\": \"PROJCS[\\\"WGS 84 / UTM zone 32N\\\",GEOGCS[\\\"WGS 84\\\",DATUM[\\\"WGS_1984\\\",SPHEROID[\\\"WGS 84\\\",6378137,298.257223563,AUTHORITY[\\\"EPSG\\\",\\\"7030\\\"]],AUTHORITY[\\\"EPSG\\\",\\\"6326\\\"]],PRIMEM[\\\"Greenwich\\\",0,AUTHORITY[\\\"EPSG\\\",\\\"8901\\\"]],UNIT[\\\"degree\\\",0.0174532925199433,AUTHORITY[\\\"EPSG\\\",\\\"9122\\\"]],AUTHORITY[\\\"EPSG\\\",\\\"4326\\\"]],PROJECTION[\\\"Transverse_Mercator\\\"],PARAMETER[\\\"latitude_of_origin\\\",0],PARAMETER[\\\"central_meridian\\\",9],PARAMETER[\\\"scale_factor\\\",0.9996],PARAMETER[\\\"false_easting\\\",500000],PARAMETER[\\\"false_northing\\\",0],UNIT[\\\"metre\\\",1,AUTHORITY[\\\"EPSG\\\",\\\"9001\\\"]],AXIS[\\\"Easting\\\",EAST],AXIS[\\\"Northing\\\",NORTH],AUTHORITY[\\\"EPSG\\\",\\\"32632\\\"]]\",\n    \"semi_major_axis\": 6378137.0,\n    \"semi_minor_axis\": 6356752.314245179,\n    \"inverse_flattening\": 298.257223563,\n    \"reference_ellipsoid_name\": \"WGS 84\",\n    \"longitude_of_prime_meridian\": 0.0,\n    \"prime_meridian_name\": \"Greenwich\",\n    \"geographic_crs_name\": \"WGS 84\",\n    \"horizontal_datum_name\": \"World Geodetic System 1984\",\n    \"projected_crs_name\": \"WGS 84 / UTM zone 32N\",\n    \"grid_mapping_name\": \"transverse_mercator\",\n    \"latitude_of_projection_origin\": 0.0,\n    \"longitude_of_central_meridian\": 9.0,\n    \"false_easting\": 500000.0,\n    \"false_northing\": 0.0,\n    \"scale_factor_at_central_meridian\": 0.9996,\n    \"spatial_ref\": \"PROJCS[\\\"WGS 84 / UTM zone 32N\\\",GEOGCS[\\\"WGS 84\\\",DATUM[\\\"WGS_1984\\\",SPHEROID[\\\"WGS 84\\\",6378137,298.257223563,AUTHORITY[\\\"EPSG\\\",\\\"7030\\\"]],AUTHORITY[\\\"EPSG\\\",\\\"6326\\\"]],PRIMEM[\\\"Greenwich\\\",0,AUTHORITY[\\\"EPSG\\\",\\\"8901\\\"]],UNIT[\\\"degree\\\",0.0174532925199433,AUTHORITY[\\\"EPSG\\\",\\\"9122\\\"]],AUTHORITY[\\\"EPSG\\\",\\\"4326\\\"]],PROJECTION[\\\"Transverse_Mercator\\\"],PARAMETER[\\\"latitude_of_origin\\\",0],PARAMETER[\\\"central_meridian\\\",9],PARAMETER[\\\"scale_factor\\\",0.9996],PARAMETER[\\\"false_easting\\\",500000],PARAMETER[\\\"false_northing\\\",0],UNIT[\\\"metre\\\",1,AUTHORITY[\\\"EPSG\\\",\\\"9001\\\"]],AXIS[\\\"Easting\\\",EAST],AXIS[\\\"Northing\\\",NORTH],AUTHORITY[\\\"EPSG\\\",\\\"32632\\\"]]\",\n    \"_ARRAY_DIMENSIONS\": [],\n    \"GeoTransform\": \"300000.0 10.0 0.0 5000040.0 0.0 -10.0\"\n    }\n  },\n  \"1/x/zarr.json\": {\n    \"zarr_format\": 3,\n    \"node_type\": \"array\",\n    \"shape\": [547],\n    \"data_type\": \"float64\",\n    \"chunk_grid\": {\"name\": \"regular\", \"configuration\": {\"chunk_shape\": [547]}},\n    \"codecs\": [{\"name\": \"bytes\"}],\n    \"dimension_names\": [\"x\"]\n  },\n  \"1/y/zarr.json\": {\n    \"zarr_format\": 3,\n    \"node_type\": \"array\",\n    \"shape\": [547],\n    \"data_type\": \"float64\",\n    \"chunk_grid\": {\"name\": \"regular\", \"configuration\": {\"chunk_shape\": [547]}},\n    \"codecs\": [{\"name\": \"bytes\"}],\n    \"dimension_names\": [\"y\"]\n  }\n}\n</code></pre> <p>This example demonstrates: - Custom CRS: Uses EPSG:32633 (UTM Zone 33N) instead of web mapping CRS - Scientific Resolution: Native 10m pixel size typical for Sentinel-2 L2A data - Custom Tile Sizes: 1024x1024 for native, 512x512 for overview to match scientific data characteristics - Consistent Structure: Both zoom levels (<code>0</code> and <code>1</code>) contain the same variables (<code>red</code>, <code>nir</code>, <code>x</code>, <code>y</code>) - Coordinate Variables: UTM coordinates in meters stored as <code>x</code> and <code>y</code> arrays - Chunk Alignment: Chunk sizes match the <code>tileWidth</code> and <code>tileHeight</code> from the TileMatrix definition</p>"},{"location":"geozarr-minispec/#file-system-hierarchy-example","title":"File System Hierarchy Example","text":"<p>The same custom CRS multiscale dataset would appear as the following directory structure on disk:</p> <pre><code>sentinel2_utm33n.zarr/\n\u251c\u2500\u2500 zarr.json                    # Root group with multiscales metadata\n\u251c\u2500\u2500 0/                          # Native resolution (10m) zoom level\n\u2502   \u251c\u2500\u2500 zarr.json               # Group metadata for zoom level 0\n\u2502   \u251c\u2500\u2500 red/                    # Red band data variable\n\u2502   \u2502   \u251c\u2500\u2500 zarr.json           # Array metadata\n\u2502   \u2502   \u2514\u2500\u2500 c/                  # Chunk directory\n\u2502   \u2502       \u251c\u2500\u2500 0/0             # Chunk files (1024x1024 chunks)\n\u2502   \u2502       \u251c\u2500\u2500 0/1\n\u2502   \u2502       \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 nir/                    # Near-infrared band data variable\n\u2502   \u2502   \u251c\u2500\u2500 zarr.json           # Array metadata\n\u2502   \u2502   \u2514\u2500\u2500 c/                  # Chunk directory\n\u2502   \u2502       \u251c\u2500\u2500 0/0             # Chunk files (1024x1024 chunks)\n\u2502   \u2502       \u251c\u2500\u2500 0/1\n\u2502   \u2502       \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 spatial_ref/            # Spatial reference system variable\n\u2502   \u2502   \u251c\u2500\u2500 zarr.json           # Array metadata with CRS information\n\u2502   \u2502   \u2514\u2500\u2500 c/                  # Chunk directory\n\u2502   \u2502       \u2514\u2500\u2500 0               # Single chunk (scalar)\n\u2502   \u251c\u2500\u2500 x/                      # X coordinate variable (UTM Easting)\n\u2502   \u2502   \u251c\u2500\u2500 zarr.json           # Array metadata\n\u2502   \u2502   \u2514\u2500\u2500 c/                  # Chunk directory\n\u2502   \u2502       \u2514\u2500\u2500 0               # Single chunk (1094 elements)\n\u2502   \u2514\u2500\u2500 y/                      # Y coordinate variable (UTM Northing)\n\u2502       \u251c\u2500\u2500 zarr.json           # Array metadata\n\u2502       \u2514\u2500\u2500 c/                  # Chunk directory\n\u2502           \u2514\u2500\u2500 0               # Single chunk (1094 elements)\n\u2514\u2500\u2500 1/                          # Overview level (20m) zoom level\n    \u251c\u2500\u2500 zarr.json               # Group metadata for zoom level 1\n    \u251c\u2500\u2500 red/                    # Red band data variable\n    \u2502   \u251c\u2500\u2500 zarr.json           # Array metadata\n    \u2502   \u2514\u2500\u2500 c/                  # Chunk directory\n    \u2502       \u251c\u2500\u2500 0/0             # Chunk files (512x512 chunks)\n    \u2502       \u251c\u2500\u2500 0/1\n    \u2502       \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 nir/                    # Near-infrared band data variable\n    \u2502   \u251c\u2500\u2500 zarr.json           # Array metadata\n    \u2502   \u2514\u2500\u2500 c/                  # Chunk directory\n    \u2502       \u251c\u2500\u2500 0/0             # Chunk files (512x512 chunks)\n    \u2502       \u251c\u2500\u2500 0/1\n    \u2502       \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 spatial_ref/            # Spatial reference system variable\n    \u2502   \u251c\u2500\u2500 zarr.json           # Array metadata with CRS information\n    \u2502   \u2514\u2500\u2500 c/                  # Chunk directory\n    \u2502       \u2514\u2500\u2500 0               # Single chunk (scalar)\n    \u251c\u2500\u2500 x/                      # X coordinate variable (UTM Easting)\n    \u2502   \u251c\u2500\u2500 zarr.json           # Array metadata\n    \u2502   \u2514\u2500\u2500 c/                  # Chunk directory\n    \u2502       \u2514\u2500\u2500 0               # Single chunk (547 elements)\n    \u2514\u2500\u2500 y/                      # Y coordinate variable (UTM Northing)\n        \u251c\u2500\u2500 zarr.json           # Array metadata\n        \u2514\u2500\u2500 c/                  # Chunk directory\n            \u2514\u2500\u2500 0               # Single chunk (547 elements)\n</code></pre> <p>Key aspects of this file system layout: - Root metadata: The <code>zarr.json</code> at the root contains the <code>multiscales</code> attribute defining the custom UTM TileMatrixSet - Zoom level groups: Directories <code>0/</code> and <code>1/</code> correspond exactly to the TileMatrix <code>id</code> values - Consistent variables: Each zoom level contains the same set of variables (<code>red</code>, <code>nir</code>, <code>x</code>, <code>y</code>) - Chunk organization: Data is stored in chunks that align with the tile dimensions specified in the TileMatrixSet - Coordinate preservation: UTM coordinates are maintained at each resolution level</p>"},{"location":"geozarr-minispec/#appendix","title":"Appendix","text":""},{"location":"geozarr-minispec/#definitions","title":"Definitions","text":""},{"location":"geozarr-minispec/#tilematrixlimit","title":"TileMatrixLimit","text":"key type required notes <code>\"tileMatrix\"</code> string yes <code>\"minTileCol\"</code> int yes <code>\"minTileRow\"</code> int yes <code>\"maxTileCol\"</code> int yes <code>\"maxTileRow\"</code> int yes"},{"location":"geozarr-minispec/#tilematrix","title":"TileMatrix","text":"key type required notes <code>\"id\"</code> string yes <code>\"scaleDenominator\"</code> float yes <code>\"cellSize\"</code> float yes <code>\"pointOfOrigin\"</code> [float, float] yes <code>\"tileWidth\"</code> int yes <code>\"tileHeight\"</code> int yes <code>\"matrixWidth\"</code> int yes <code>\"matrixHeight\"</code> int yes"},{"location":"geozarr-minispec/#tilematrixset","title":"TileMatrixSet","text":"key type required notes <code>\"id\"</code> string yes <code>\"title\"</code> string no <code>\"crs\"</code> string no <code>\"supportedCRS\"</code> string no <code>\"orderedAxes\"</code> [str, str] no <code>\"tileMatrices\"</code> [TileMatrix, ...] yes May not be empty"},{"location":"geozarr-minispec/#resamplingmethod","title":"ResamplingMethod","text":"<p>This is a string literal defined here.</p> <p>The implementation defaults to <code>\"average\"</code> for creating overview levels in multiscale datasets.</p>"},{"location":"geozarr-specification-contribution/","title":"GeoZarr Specification Contribution","text":"<p>This document outlines our contribution to the GeoZarr specification based on our implementation experience with the EOPF GeoZarr data model.</p>"},{"location":"geozarr-specification-contribution/#overview","title":"Overview","text":"<p>Our implementation of GeoZarr-compliant data conversion for Earth Observation data has revealed several areas where the current specification could be improved to better support scientific use cases. We have contributed feedback to the GeoZarr specification development process through detailed GitHub issues.</p> <p>Our implementation follows the GeoZarr Mini Spec, which documents the specific subset of the GeoZarr specification that we implement, including implementation-specific details for chunking requirements, CF compliance, and multiscale dataset organization.</p>"},{"location":"geozarr-specification-contribution/#key-issues-identified-and-reported","title":"Key Issues Identified and Reported","text":""},{"location":"geozarr-specification-contribution/#1-arbitrary-coordinate-systems-support","title":"1. Arbitrary Coordinate Systems Support","text":"<p>Issue: zarr-developers/geozarr-spec#81</p> <p>Problem: The current specification has an implicit bias toward web mapping tile schemes (WebMercatorQuad), which may discourage scientific applications that work with native coordinate reference systems.</p> <p>Our Solution: Our implementation successfully demonstrates:</p> <ul> <li>Creation of \"Native CRS Tile Matrix Sets\" for arbitrary projections</li> <li>Multiscale pyramids working with UTM and other scientific projections</li> <li>Proper scale denominator calculations for non-web CRS</li> <li>Chunking strategies optimized for native coordinate systems</li> </ul> <p>Impact: This is critical for Earth observation data that often comes in UTM zones, polar stereographic, or other scientific projections where preserving the native CRS maintains scientific accuracy.</p>"},{"location":"geozarr-specification-contribution/#2-chunking-performance-optimization","title":"2. Chunking Performance Optimization","text":"<p>Issue: zarr-developers/geozarr-spec#82</p> <p>Problem: The specification requires strict 1:1 mapping between Zarr chunks and tile matrix tiles, which prevents optimal chunking strategies for different data types and storage backends.</p> <p>Our Solution: We implemented sophisticated chunk alignment logic:</p> <pre><code>def calculate_aligned_chunk_size(dimension_size: int, target_chunk_size: int) -&gt; int:\n    \"\"\"Calculate a chunk size that divides evenly into the dimension size.\"\"\"\n    if target_chunk_size &gt;= dimension_size:\n        return dimension_size\n\n    # Find the largest divisor that is &lt;= target_chunk_size\n    for chunk_size in range(target_chunk_size, 0, -1):\n        if dimension_size % chunk_size == 0:\n            return chunk_size\n    return 1\n</code></pre> <p>Impact: This approach prevents chunk overlap issues with Dask while optimizing for actual data dimensions rather than arbitrary tile sizes, significantly improving performance.</p>"},{"location":"geozarr-specification-contribution/#3-multiscale-hierarchy-structure-clarification","title":"3. Multiscale Hierarchy Structure Clarification","text":"<p>Issue: zarr-developers/geozarr-spec#83</p> <p>Problem: The specification describes multiscale encoding but doesn't clearly define the exact hierarchical structure and relationship between parent groups and zoom level children.</p> <p>Our Solution: We implemented a clear hierarchy structure:</p> <pre><code>/measurements/r10m/          # Parent group with multiscales metadata\n\u251c\u2500\u2500 0/                       # Native resolution (zoom level 0)\n\u2502   \u251c\u2500\u2500 band1\n\u2502   \u251c\u2500\u2500 band2\n\u2502   \u2514\u2500\u2500 spatial_ref\n\u251c\u2500\u2500 1/                       # First overview level\n\u2502   \u251c\u2500\u2500 band1\n\u2502   \u251c\u2500\u2500 band2\n\u2502   \u2514\u2500\u2500 spatial_ref\n\u2514\u2500\u2500 2/                       # Second overview level\n    \u251c\u2500\u2500 band1\n    \u251c\u2500\u2500 band2\n    \u2514\u2500\u2500 spatial_ref\n</code></pre> <p>Impact: This provides a concrete, tested pattern for implementing multiscale hierarchies that other implementations can follow.</p>"},{"location":"geozarr-specification-contribution/#implementation-evidence","title":"Implementation Evidence","text":"<p>Our implementation provides concrete evidence for these improvements:</p>"},{"location":"geozarr-specification-contribution/#native-crs-preservation","title":"Native CRS Preservation","text":"<ul> <li>Function: <code>create_native_crs_tile_matrix_set()</code></li> <li>Purpose: Creates custom tile matrix sets for arbitrary coordinate reference systems</li> <li>Benefit: Maintains scientific accuracy without unnecessary reprojection</li> </ul>"},{"location":"geozarr-specification-contribution/#robust-processing","title":"Robust Processing","text":"<ul> <li>Function: <code>write_dataset_band_by_band_with_validation()</code></li> <li>Purpose: Handles large datasets with retry logic and validation</li> <li>Benefit: Production-ready robustness for real-world data processing</li> </ul>"},{"location":"geozarr-specification-contribution/#comprehensive-metadata-handling","title":"Comprehensive Metadata Handling","text":"<ul> <li>Function: <code>_add_coordinate_metadata()</code></li> <li>Purpose: Handles diverse coordinate types (time, angle, band, detector)</li> <li>Benefit: Supports the full range of Earth observation data structures</li> </ul>"},{"location":"geozarr-specification-contribution/#cloud-storage-optimization","title":"Cloud Storage Optimization","text":"<ul> <li>Features: S3 support with credential validation, storage options handling</li> <li>Benefit: Enables cloud-native workflows with proper error handling</li> </ul>"},{"location":"geozarr-specification-contribution/#specification-sections-addressed","title":"Specification Sections Addressed","text":"<p>Our contributions target specific sections of the GeoZarr specification:</p> <ul> <li>Section 9.7.3 (Tile Matrix Set Representation) - Native CRS support</li> <li>Section 9.7.4 (Chunk Layout Alignment) - Flexible chunking</li> <li>Section 9.7.1 (Hierarchical Layout) - Clear structure definition</li> <li>Section 9.7.2 (Metadata Encoding) - Metadata placement guidance</li> </ul>"},{"location":"geozarr-specification-contribution/#benefits-for-the-earth-observation-community","title":"Benefits for the Earth Observation Community","text":"<p>These contributions specifically benefit Earth observation and scientific data applications:</p> <ol> <li>Scientific Accuracy: Preserving native CRS prevents distortion from unnecessary reprojections</li> <li>Performance: Optimized chunking improves processing speed and reduces memory usage</li> <li>Clarity: Clear hierarchy definitions enable consistent implementations</li> <li>Robustness: Production patterns support real-world deployment scenarios</li> </ol>"},{"location":"geozarr-specification-contribution/#future-work","title":"Future Work","text":"<p>We continue to monitor the specification development and will contribute additional feedback as our implementation evolves. Areas for potential future contribution include:</p> <ul> <li>Cloud storage optimization patterns</li> <li>Coordinate variable handling for diverse data types</li> <li>Integration with STAC metadata standards</li> <li>Guidance for time dimension handling</li> </ul>"},{"location":"geozarr-specification-contribution/#related-documentation","title":"Related Documentation","text":"<ul> <li>Converter Documentation - Technical details of our implementation</li> <li>Architecture - Technical architecture and design principles</li> <li>API Reference - Complete Python API documentation</li> </ul>"},{"location":"geozarr-specification-contribution/#links","title":"Links","text":"<ul> <li>GeoZarr Specification Repository</li> <li>Our GitHub Issues</li> <li>Project Issue #74</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>This guide covers the installation of the EOPF GeoZarr library and its dependencies.</p>"},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11 or higher</li> <li>Operating System: Linux, macOS, or Windows</li> </ul>"},{"location":"installation/#installation-methods","title":"Installation Methods","text":""},{"location":"installation/#using-pip-recommended","title":"Using pip (Recommended)","text":"<p>Install the latest stable version from PyPI:</p> <pre><code>pip install eopf-geozarr\n</code></pre>"},{"location":"installation/#using-uv-fast-alternative","title":"Using uv (Fast Alternative)","text":"<p>If you have uv installed:</p> <pre><code>uv add eopf-geozarr\n</code></pre>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<p>For development or to get the latest features:</p> <pre><code>git clone https://github.com/eopf-explorer/data-model.git\ncd data-model\npip install -e .\n</code></pre>"},{"location":"installation/#dependencies","title":"Dependencies","text":"<p>The library automatically installs the following key dependencies:</p> <ul> <li>pydantic-zarr (\u22650.8.0) - Zarr data validation</li> <li>zarr (\u22653.1.1) - Zarr format support</li> <li>xarray (\u22652025.7.1) - N-dimensional labeled arrays</li> <li>dask (\u22652025.5.1) - Parallel computing</li> <li>rioxarray (\u22650.13.0) - Geospatial xarray extension</li> <li>s3fs (\u22652024.6.0) - S3 filesystem support</li> <li>pyproj (\u22653.7.0) - Cartographic projections</li> </ul>"},{"location":"installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"installation/#development-tools","title":"Development Tools","text":"<p>For development work, install additional tools:</p> <pre><code>pip install eopf-geozarr[dev]\n</code></pre> <p>This includes:</p> <ul> <li>Testing frameworks (pytest, pytest-cov)</li> <li>Code formatting (black, isort)</li> <li>Linting (flake8, mypy)</li> <li>Security scanning (bandit, safety)</li> </ul>"},{"location":"installation/#documentation","title":"Documentation","text":"<p>To build documentation locally:</p> <pre><code>pip install eopf-geozarr[docs]\n</code></pre>"},{"location":"installation/#verification","title":"Verification","text":"<p>Verify your installation by running:</p> <pre><code>eopf-geozarr --version\n</code></pre> <p>Or in Python:</p> <pre><code>import eopf_geozarr\nprint(eopf_geozarr.__version__)\n</code></pre>"},{"location":"installation/#cloud-storage-setup","title":"Cloud Storage Setup","text":""},{"location":"installation/#aws-s3-configuration","title":"AWS S3 Configuration","text":"<p>For S3 support, configure your AWS credentials:</p> <pre><code># Using AWS CLI\naws configure\n\n# Or set environment variables\nexport AWS_ACCESS_KEY_ID=your_access_key\nexport AWS_SECRET_ACCESS_KEY=your_secret_key\nexport AWS_DEFAULT_REGION=us-east-1\n</code></pre>"},{"location":"installation/#alternative-s3-compatible-storage","title":"Alternative S3-Compatible Storage","text":"<p>For other S3-compatible services (MinIO, DigitalOcean Spaces, etc.):</p> <pre><code>export AWS_ENDPOINT_URL=https://your-endpoint.com\nexport AWS_ACCESS_KEY_ID=your_access_key\nexport AWS_SECRET_ACCESS_KEY=your_secret_key\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#common-issues","title":"Common Issues","text":"<p>ImportError: No module named 'eopf_geozarr'</p> <ul> <li>Ensure you're using the correct Python environment</li> <li>Verify installation with <code>pip list | grep eopf-geozarr</code></li> </ul> <p>Permission errors during installation</p> <ul> <li>Use <code>pip install --user eopf-geozarr</code> for user-level installation</li> <li>Or use a virtual environment (recommended)</li> </ul> <p>Dependency conflicts</p> <ul> <li>Create a fresh virtual environment</li> <li>Use <code>pip install --upgrade eopf-geozarr</code> to update dependencies</li> </ul>"},{"location":"installation/#virtual-environment-setup","title":"Virtual Environment Setup","text":"<p>Recommended approach using venv:</p> <pre><code>python -m venv eopf-env\nsource eopf-env/bin/activate  # On Windows: eopf-env\\Scripts\\activate\npip install eopf-geozarr\n</code></pre>"},{"location":"installation/#system-specific-notes","title":"System-Specific Notes","text":"<p>macOS with Apple Silicon</p> <ul> <li>Some dependencies may require Rosetta 2 or native ARM builds</li> <li>Consider using conda for better compatibility</li> </ul> <p>Windows</p> <ul> <li>Ensure Visual C++ Build Tools are installed for some dependencies</li> <li>Use Windows Subsystem for Linux (WSL) for best compatibility</li> </ul>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>After installation, proceed to the Quick Start guide to begin using the library.</p>"},{"location":"quickstart/","title":"Quick Start","text":"<p>Get up and running with EOPF GeoZarr in minutes. This guide shows you how to convert your first EOPF dataset to GeoZarr format.</p>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>EOPF GeoZarr library installed (Installation Guide)</li> <li>An EOPF dataset in Zarr format</li> <li>Basic familiarity with Python and command-line tools</li> </ul>"},{"location":"quickstart/#your-first-conversion","title":"Your First Conversion","text":""},{"location":"quickstart/#command-line-simplest","title":"Command Line (Simplest)","text":"<p>Convert an EOPF dataset to GeoZarr format:</p> <pre><code>eopf-geozarr convert input.zarr output.zarr\n</code></pre> <p>That's it! The converter will:</p> <ul> <li>Analyze your EOPF dataset structure</li> <li>Apply GeoZarr 0.4 specification compliance</li> <li>Create multiscale overviews</li> <li>Preserve native CRS and scientific accuracy</li> </ul>"},{"location":"quickstart/#python-api-more-control","title":"Python API (More Control)","text":"<p>For programmatic usage with custom parameters:</p> <pre><code># test: skip\nimport xarray as xr\nfrom eopf_geozarr import create_geozarr_dataset\n\n# Load your EOPF DataTree\ndt = xr.open_datatree(\"input.zarr\", engine=\"zarr\")\n\n# Convert to GeoZarr\ndt_geozarr = create_geozarr_dataset(\n    dt_input=dt,\n    groups=[\"/measurements/r10m\", \"/measurements/r20m\", \"/measurements/r60m\"],\n    output_path=\"output.zarr\",\n    spatial_chunk=4096,\n    min_dimension=256\n)\n\nprint(\"Conversion complete!\")\n</code></pre>"},{"location":"quickstart/#working-with-cloud-storage","title":"Working with Cloud Storage","text":""},{"location":"quickstart/#s3-output","title":"S3 Output","text":"<p>Save directly to AWS S3:</p> <pre><code># Set credentials\nexport AWS_ACCESS_KEY_ID=your_key\nexport AWS_SECRET_ACCESS_KEY=your_secret\nexport AWS_DEFAULT_REGION=us-east-1\n\n# Convert to S3\neopf-geozarr convert input.zarr s3://my-bucket/output.zarr\n</code></pre>"},{"location":"quickstart/#s3-input-and-output","title":"S3 Input and Output","text":"<pre><code># Both input and output on S3\ndt_geozarr = create_geozarr_dataset(\n    dt_input=xr.open_datatree(\"s3://input-bucket/data.zarr\", engine=\"zarr\"),\n    groups=[\"/measurements/r10m\"],\n    output_path=\"s3://output-bucket/geozarr.zarr\"\n)\n</code></pre>"},{"location":"quickstart/#validation","title":"Validation","text":"<p>Verify your GeoZarr dataset meets the specification:</p> <pre><code>eopf-geozarr validate output.zarr\n</code></pre> <p>Or in Python:</p> <pre><code>from eopf_geozarr.cli import validate_command\nimport argparse\n\n# Create args object\nargs = argparse.Namespace()\nargs.input_path = \"output.zarr\"\nargs.verbose = True\n\nvalidate_command(args)\n</code></pre>"},{"location":"quickstart/#inspecting-results","title":"Inspecting Results","text":""},{"location":"quickstart/#dataset-information","title":"Dataset Information","text":"<p>Get detailed information about your converted dataset:</p> <pre><code>eopf-geozarr info output.zarr\n</code></pre>"},{"location":"quickstart/#python-inspection","title":"Python Inspection","text":"<pre><code>import xarray as xr\n\n# Open the converted dataset\ndt = xr.open_datatree(\"output.zarr\", engine=\"zarr\")\n\n# Explore the structure\nprint(dt)\n\n# Check multiscales metadata\nprint(dt.attrs.get('multiscales', 'No multiscales found'))\n\n# Examine resolution levels\n# Note: Structure depends on converter version (see converter.md for V0 vs V1 differences)\n# V0 (deprecated): /measurements/r10m/0, /measurements/r10m/1, etc.\n# V1 (current): /measurements/reflectance/r10m, /measurements/reflectance/r20m, etc.\n\n# Example for V0 structure:\nif \"/measurements/r10m/0\" in dt.groups:\n    ds_native = dt[\"/measurements/r10m/0\"].ds\n    print(f\"Native shape: {ds_native.dims}\")\n\n# Example for V1 structure:\nif \"/measurements/reflectance/r10m\" in dt.groups:\n    ds_10m = dt[\"/measurements/reflectance/r10m\"].ds\n    ds_20m = dt[\"/measurements/reflectance/r20m\"].ds\n    print(f\"10m resolution: {ds_10m.dims}\")\n    print(f\"20m resolution: {ds_20m.dims}\")\n</code></pre>"},{"location":"quickstart/#common-patterns","title":"Common Patterns","text":""},{"location":"quickstart/#sentinel-2-data","title":"Sentinel-2 Data","text":"<p>For Sentinel-2 L2A data, use the optimized V1 converter (recommended):</p> <pre><code>from eopf_geozarr.s2_optimization.s2_converter import convert_s2_optimized\n\n# Recommended: Use V1 optimized converter for Sentinel-2\ndt_optimized = convert_s2_optimized(\n    dt_input=dt,\n    output_path=\"s2_optimized.zarr\",\n    spatial_chunk=256,\n    enable_sharding=True\n)\n</code></pre> <p>The V1 converter automatically: - Reuses native resolutions (r10m, r20m, r60m) without duplication - Adds coarser levels (r120m, r360m, r720m) for efficient visualization - Applies variable-aware resampling for different data types</p> <p>Note: For details on V0 vs V1 differences, see the converter documentation.</p>"},{"location":"quickstart/#large-datasets-with-dask","title":"Large Datasets with Dask","text":"<p>For processing large datasets efficiently:</p> <pre><code>eopf-geozarr convert large_input.zarr output.zarr --dask-cluster\n</code></pre> <p>Or in Python:</p> <pre><code>from dask.distributed import Client\n\n# Start Dask client\nclient = Client('scheduler-address:8786')  # Or Client() for local\n\n# Process with Dask\ndt_geozarr = create_geozarr_dataset(\n    dt_input=dt,\n    groups=[\"/measurements/r10m\"],\n    output_path=\"output.zarr\",\n    spatial_chunk=2048  # Smaller chunks for distributed processing\n)\n\nclient.close()\n</code></pre>"},{"location":"quickstart/#key-features-demonstrated","title":"Key Features Demonstrated","text":"<p>Your converted dataset now includes:</p> <p>\u2705 GeoZarr 0.4 Compliance - Full specification adherence \u2705 Native CRS Preservation - No unnecessary reprojection \u2705 Multiscale Pyramids - Efficient overview levels \u2705 Optimized Chunking - Aligned chunks for performance \u2705 CF Conventions - Standard metadata attributes \u2705 Cloud-Ready - S3 and other cloud storage support  </p>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Detailed Usage: See the User Guide for advanced options</li> <li>API Reference: Explore the API Reference for all functions</li> <li>Examples: Check out Examples for specific use cases</li> <li>Architecture: Understand the Architecture behind the conversion</li> </ul>"},{"location":"quickstart/#troubleshooting-quick-fixes","title":"Troubleshooting Quick Fixes","text":"<p>Memory errors with large datasets?</p> <pre><code>eopf-geozarr convert input.zarr output.zarr --spatial-chunk 2048\n</code></pre> <p>S3 permission errors?</p> <pre><code>aws sts get-caller-identity  # Verify credentials\n</code></pre> <p>Validation failures?</p> <pre><code>eopf-geozarr validate output.zarr --verbose  # Get detailed error info\n</code></pre> <p>For more troubleshooting help, see the FAQ.</p>"}]}